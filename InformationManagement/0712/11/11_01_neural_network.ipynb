{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbqkmaY9RA-z"
      },
      "source": [
        "# 第11回 その1: 中間層が1層のニューラルネットワークの実装\n",
        "10_03_neural_network.ipynbでは中間層が1層でノード数が2のニューラルネットワークを実装しました。  \n",
        "ここでは，中間層が1層で，ノード数が可変にできるようにしたソースコードを示します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1DmeW6oRs8r"
      },
      "source": [
        "## ステップ0: Google Driveのマウントと作業フォルダへの移動  \n",
        "Google Drive に配置したデータを読み込むための準備です。  \n",
        "詳細については第二回の 02_01_graph.ipynb を参照してください。  \n",
        "\n",
        "ここでは\"マイドライブ/情報管理/11\"を作業フォルダとします。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4b4cfHPRXML"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# フォルダの移動には\"%cd\"を使用します。\n",
        "# 作業フォルダへ移動\n",
        "%cd /content/drive/'My Drive'/情報管理/11/\n",
        "# 現在のフォルダの中身を表示\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF6-hyJ2Sdau"
      },
      "source": [
        "`2class_data_circle.csv`というデータが表示されていることを確認してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zJI0HyoWGGc"
      },
      "source": [
        "## ステップ1: データの読み込みと標準化\n",
        "まずは必要ライブラリをインポートします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYfdOuD_WNd_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXb_O89VWQQY"
      },
      "source": [
        "`2class_data_circle.csv` を読み込みます。  \n",
        "このデータはx1とx2の二次元データで，クラス0とクラス1の2クラスのどちらかに属しています。  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkibCXvsRv-8"
      },
      "outputs": [],
      "source": [
        "# pandas の関数 read_csv を用いた csvファイル読み込み\n",
        "csv_data = pd.read_csv('2class_data_circle.csv', encoding='SHIFT-JIS')\n",
        "\n",
        "# データの前半部(.headで取得できる)のみ表示\n",
        "display(csv_data.head())\n",
        "\n",
        "# x1とx2のデータを抽出し，numpy用データ(ndarray型) に変換する。\n",
        "X = csv_data.loc[:, ['x1','x2']].to_numpy()\n",
        "\n",
        "# クラス番号を抽出し，numpy用データ(ndarray型) に変換する。\n",
        "Y = csv_data.loc[:,'class'].to_numpy()\n",
        "\n",
        "# データのサンプル数と次元数を得る。\n",
        "(num_samples, num_dimensions) = np.shape(X)\n",
        "print('Nunber of samples: ' + str(num_samples))\n",
        "print('Number of dimensions: ' + str(num_dimensions))\n",
        "\n",
        "# クラス数を得る。\n",
        "num_classes = int(np.max(Y) + 1)\n",
        "print('Number of classes: ' + str(num_classes))\n",
        "\n",
        "# データを標準化する。\n",
        "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
        "\n",
        "color_list = [(0, 0, 1), (1, 0, 0)]\n",
        "# データをプロット\n",
        "plt.figure(figsize=(7,7))\n",
        "for k in range(num_classes):\n",
        "  # n番目のクラスに属している(Y==c)データをプロット\n",
        "  plt.scatter(X[Y==k,0], X[Y==k,1], color=color_list[k], label='class'+str(k))\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.legend()\n",
        "plt.xlim([-3, 3])\n",
        "plt.ylim([-3, 3])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "いま，yには各データの正解クラス番号(0 or 1)が格納されていますが，  \n",
        "クロスエントロピー損失の計算や勾配の計算には，クラス番号ではなく，各クラスの正解の確率が必要です。  \n",
        "そのため，正解クラスは確率=1，不正解クラスは確率=0となるベクトルに変換しておきます。これを<font color=red>**one-hotベクトル**</font>と呼びます。"
      ],
      "metadata": {
        "id": "zM09PAv5304b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ゼロ行列を生成\n",
        "Y_onehot = np.zeros([num_samples, num_classes])\n",
        "for n in range(num_samples):\n",
        "  # クラス番号(y[n])に対応するインデクスを1にする\n",
        "  Y_onehot[n, Y[n]] = 1\n",
        "\n",
        "# 先頭の5サンプル分を表示\n",
        "print('class Y:')\n",
        "print(Y[:5])\n",
        "print('one-hot vector Y_onehot:')\n",
        "print(Y_onehot[:5])"
      ],
      "metadata": {
        "id": "1M957FOX4c07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yが[0, 1, 1, 0, 1]なのに対して，Y_onehotの各行は0, 1, 1, 0, 1番目の要素が1に，それ以外が0になっているのが分かります。"
      ],
      "metadata": {
        "id": "ctNd28Xu5FIv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxl2IKgJWiM-"
      },
      "source": [
        "## ステップ2: 中間層のノード数が可変の3層ニューラルネットワークの実装  \n",
        "10_03_neural_network.ipynbでは，中間層のノード数が2と限定した実装になっていましたが，  \n",
        "ここではノード数を可変にできるような実装にします。  \n",
        "\n",
        "入力の次元数を$D$，クラス数を$K$，中間層のノード数を$J$としたときのニューラルネットワークの式を以下のように定義する。  \n",
        "$\\left[\\begin{matrix}g_{1}\\\\\\vdots\\\\g_{J} \\end{matrix} \\right] = \n",
        "\\left[\\begin{matrix} w_{11}^{\\rm mid}x_1 +\\cdots+ w_{1D}^{\\rm mid}x_D + b_{1}^{\\rm mid}\\\\ \\vdots \\\\ w_{J1}^{\\rm mid}x_1 +\\cdots+ w_{JD}^{\\rm mid}x_D + b_{J}^{\\rm mid} \\end{matrix} \\right] = \n",
        "\\left[\\begin{matrix} w_{11}^{\\rm mid} &\\cdots& w_{1D}^{\\rm mid} \\\\  & \\vdots& \\\\ w_{J1}^{\\rm mid} &\\cdots& w_{JD}^{\\rm mid} \\end{matrix} \\right]\\left[\\begin{matrix}x_1\\\\\\vdots\\\\ x_D\\end{matrix} \\right] + \\left[\\begin{matrix}b_{1}^{\\rm mid}\\\\\\vdots\\\\b_{J}^{\\rm mid} \\end{matrix} \\right] = {\\bf W}^{\\rm mid}{\\bf x} + {\\bf b}^{\\rm mid}$  \n",
        "$\\left[\\begin{matrix}h_{1}\\\\\\vdots\\\\h_{J} \\end{matrix} \\right] = \n",
        "\\left[\\begin{matrix}{\\rm sigmoid}(g_1) \\\\ \\vdots \\\\ {\\rm sigmoid}(g_J)\\end{matrix} \\right]$  \n",
        "$\\left[\\begin{matrix}z_{1}\\\\\\vdots\\\\z_{K} \\end{matrix} \\right] = \n",
        "\\left[\\begin{matrix} w_{11}h_1 +\\cdots+ w_{1J}h_J + b_{1}\\\\ \\vdots \\\\ w_{K1}h_1 +\\cdots+ w_{KJ}h_J + b_{K} \\end{matrix} \\right] = \n",
        "\\left[\\begin{matrix} w_{11} & \\cdots & w_{1J} \\\\ & \\vdots &  \\\\ w_{K1} & \\cdots & w_{KJ} \\end{matrix} \\right]\\left[\\begin{matrix}h_1\\\\\\vdots\\\\h_J \\end{matrix} \\right] + \\left[\\begin{matrix}b_{1}\\\\\\vdots\\\\b_{K} \\end{matrix} \\right] = {\\bf W}{\\bf h} + {\\bf b}$  \n",
        "$\\left[\\begin{matrix}\\hat{y}_{1}\\\\\\vdots\\\\\\hat{y}_{K} \\end{matrix} \\right] = \n",
        "\\left[\\begin{matrix}{\\rm softmax}(z_1) \\\\ \\vdots \\\\ {\\rm softmax}(z_K)\\end{matrix} \\right]$\n",
        "\n",
        "${\\bf W}^{\\rm mod}$ および ${\\bf b}^{\\rm mod}$ はそれぞれ中間層の重み行列とバイアス項で，サイズは $J \\times D$，$J$ です。    \n",
        "${\\bf W}$ および ${\\bf b}$ は出力層の重み行列とバイアス項で，サイズは $K \\times J$，$K$ です。 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RmuM3aYnanP"
      },
      "source": [
        "上式に従って，1サンプルデータに対してニューラルネットワークを通し，$\\bf \\hat{y}$ を計算する関数を以下に定義します。  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkdO-S8SuDPX"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "  '''\n",
        "      シグモイド関数\n",
        "  '''\n",
        "  return (1+np.exp(-1*z))**(-1)\n",
        "\n",
        "def softmax(z):\n",
        "  '''\n",
        "      ソフトマックス関数\n",
        "      z: 要素数=クラス数のベクトル\n",
        "      y_hat: クラス毎の確率。要素数=クラス数のベクトル\n",
        "  '''\n",
        "  return (np.exp(z) / np.sum(np.exp(z)))\n",
        "\n",
        "def neural_network(x, W_mid, b_mid, W, b):\n",
        "  '''\n",
        "      1サンプルデータに対してニューラルネットワークによるクラス分類を行う\n",
        "      x: 1サンプルデータ。サイズ=Dのベクトル\n",
        "      W_mid: 中間層の重み行列。サイズ=(JxD)の行列\n",
        "      b_mid: 中間層のバイアス。サイズ=Jのベクトル\n",
        "      W: 出力層の重み行列。サイズ=(KxJ)の行列\n",
        "      b: 出力層のバイアス。サイズ=Kの行列\n",
        "      ただし，D:次元数，J:中間層のノード数，K:クラス数である。\n",
        "      y_hat: 出力層の出力。サイズ=Kのベクトル\n",
        "      h: 中間層の出力。サイズ=Jのベクトル\n",
        "  '''\n",
        "  # 中間層を計算\n",
        "  g = np.dot(W_mid, x) + b_mid\n",
        "  h = sigmoid(g)\n",
        "\n",
        "  # 出力層を計算\n",
        "  z = np.dot(W, h) + b\n",
        "  y_hat = softmax(z)\n",
        "\n",
        "  return y_hat, h\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10_03_neural_network.ipynbではneural_network関数の`w_mid_1`を$\\left[\\begin{matrix} w_{11}^{\\rm mid} & w_{12}^{\\rm mid} \\end{matrix}\\right]$，`w_mid_2`を$\\left[\\begin{matrix} w_{21}^{\\rm mid} & w_{22}^{\\rm mid} \\end{matrix}\\right]$と二つのベクトルに分けて定義していましたが，今回は  \n",
        "変数`W_mid` を${\\bf W}^{\\rm mid}=\\left[\\begin{matrix} w_{11}^{\\rm mid} &\\cdots& w_{1D}^{\\rm mid} \\\\  & \\vdots& \\\\ w_{J1}^{\\rm mid} &\\cdots& w_{JD}^{\\rm mid} \\end{matrix} \\right]$という行列で定義しています。  \n",
        "またバイアス項に関しても，10_03_neural_network.ipynbでは$b_{1}^{\\rm mid}, b_{2}^{\\rm mid}$をそれぞれ`b_mid_1`，`b_mid_2`という二つのスカラー値に分けて定義していましたが，今回は`b_mid`を$\\left[\\begin{matrix} b_{1}^{\\rm mid} & b_{2}^{\\rm mid} \\end{matrix}\\right]$というベクトルで定義しています。  \n",
        "`W_mid`と`b_mid`がそれぞれベクトル → 行列，スカラ → ベクトル，と変わっても，numpy.dot関数で適切に内積処理をしてくれます。   \n",
        "出力層に関しては変更していません。"
      ],
      "metadata": {
        "id": "Cha_GuHdAG7G"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu0ihSZ-tPcX"
      },
      "source": [
        "クロスエントロピー損失$L_{ce}$を計算する関数および勾配を計算する関数を定義します。  \n",
        "勾配の計算式は以下のように定義されます。  \n",
        "出力層の勾配:  \n",
        "$\\frac{\\partial L_{ce}}{\\partial w_{kj}} = (\\hat{y}_k - y_k)h_j$  \n",
        "$\\frac{\\partial L_{ce}}{\\partial b_{k}} = \\hat{y}_k - y_k$  \n",
        "中間層の勾配:  \n",
        "$\\frac{\\partial L_{ce}}{\\partial W_{jd}^{\\rm mod}} = \\left [ \\sum_{k=1}^{K}(\\hat{y}_k - y_k)w_{kj} \\right ] (1-h_j)h_{j}x_{d}$   \n",
        "$\\frac{\\partial L_{ce}}{\\partial b_{j}^{\\rm mod}} = \\left [ \\sum_{k=1}^{K}(\\hat{y}_k - y_k)w_{kj} \\right ] (1-h_j)h_j$  \n",
        "$k$は出力層のノード(=クラス)の番号($1 \\le k \\le K$)  \n",
        "$j$は中間層のノードの番号($1 \\le j \\le J$)  \n",
        "$d$は入力層のノード(=入力データの次元)の番号($1 \\le d \\le D)$  \n",
        "です。\n",
        "\n",
        "$y_k$は正解ラベルで，$k$が正解クラスの場合は確率1，不正解クラスの場合は0です。  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G33yRYprRsh"
      },
      "outputs": [],
      "source": [
        "def cross_entropy(y_hat, y_onehot):\n",
        "  '''\n",
        "      クロスエントロピー損失を計算する。\n",
        "      y_hat: マルチクラス回帰によって推定された y\n",
        "      y_onehot: 正解のクラスの確率が1，それ以外の確率を0とするベクトル\n",
        "                (one-hotベクトル)\n",
        "  '''\n",
        "  return -1.0 * np.sum(y_onehot*np.log(y_hat))\n",
        "\n",
        "\n",
        "def calc_gradient(y_hat, y_onehot, h, W, x):\n",
        "  '''\n",
        "      勾配を計算する\n",
        "      y_hat: 出力層の出力: サイズ=Kのベクトル\n",
        "      y_onehot: 正解のクラスの確率が1，それ以外の確率を0とするベクトル\n",
        "                (one-hotベクトル)\n",
        "      h: 中間層の出力。サイズ=Jのベクトル\n",
        "      W: 出力層の重み行列。サイズ=(KxJ)の行列\n",
        "      x: データ: サイズ=Dのベクトル\n",
        "      ただし，D:次元数，J:中間層のノード数，K:クラス数である。\n",
        "      grad_W_mid: W_midの勾配：サイズ=(JxD)の行列\n",
        "      grad_b_mid: b_midの勾配：サイズ=Jのベクトル\n",
        "      grad_W: Wの勾配: サイズ=(KxJ)の行列\n",
        "      grad_b: bの勾配: サイズ=Kのベクトル\n",
        "  '''\n",
        "  # 次元数D, 中間層のノード数J, クラス数Kを得る。\n",
        "  D = np.size(x)\n",
        "  J = np.size(h)\n",
        "  K = np.size(y_hat)\n",
        "\n",
        "  # 出力層の勾配を計算\n",
        "  prev_grad = y_hat - y_onehot\n",
        "  grad_W = np.dot(np.array([prev_grad]).T, np.array([h]))\n",
        "  grad_b = prev_grad\n",
        "\n",
        "  # 中間層の勾配を計算\n",
        "  prev_grad = np.dot(prev_grad, W)*(1 - h)*h\n",
        "  grad_W_mid = np.dot(np.array([prev_grad]).T, np.array([x]))\n",
        "  grad_b_mid = prev_grad\n",
        "\n",
        "  return grad_W_mid, grad_b_mid, grad_W, grad_b"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "関数'calc_gradient'について解説します。  \n",
        "まず出力層の勾配についてです。  \n",
        "最初の  \n",
        "`prev_grad = y_hat - y_onehot`  \n",
        "では，$\\hat{y}_k - y_k$を各$k$について計算しています。  \n",
        "すなわち`prev_grad`はサイズ=$K$のベクトルです。  \n",
        "\n",
        "次の  \n",
        "`grad_W = np.dot(np.array([prev_grad]).T, np.array([self.h[layer-1]]))`  \n",
        "は，以下の処理を1行で書いたことに相当します。  \n",
        "```\n",
        "grad_W = np.zeros([K, J])\n",
        "for k in range(K):\n",
        "  for j in range(J):\n",
        "    grad_W[k, j] = prev_grad[k] * h[j]\n",
        "```\n",
        "上記は$\\frac{\\partial L_{ce}}{\\partial w_{kj}} = (\\hat{y}_k - y_k)h_j$の計算をforループを使って各$k$，$j$に対して行うコードです。  \n",
        "この処理を，行列形式でまとめて記述とこうなります。  \n",
        "$\\frac{\\partial L_{ce}}{\\partial {\\bf W}} = \n",
        "\\left[\\begin{matrix} \\hat{y}_{1} - y_{1} \\\\ \\vdots \\\\ \\hat{y}_{K} - y_{K}\\end{matrix} \\right]\n",
        "\\left[\\begin{matrix} h_{1} & \\cdots & h_{J}\\end{matrix} \\right]\n",
        "$  \n",
        "これをpythonで記述すると  \n",
        "`grad_W = np.dot(np.array([prev_grad]).T, np.array([h]))`となります。  \n",
        "`np.array([prev_grad])`および`np.array([h])`は，要素数$K$のベクトル`prev_grad`と要素数$J$のベクトル`h`を，それぞれ$(1\\times K)$，$(1\\times J)$の行列に変えたことを意味します。  \n",
        "その後，`np.array([prev_grad]).T`とすることで，`prev_grad`を$(K\\times 1)$の行列に転置し，さらに$(1\\times J)$に変換された`h`と内積を取っています。  \n",
        "\n",
        "続いて中間層の勾配についてです。  \n",
        "`prev_grad = np.dot(prev_grad, W)*(1 - h)*h`  \n",
        "では，中間層のWおよびbの勾配計算式の共通部分である  \n",
        "$\\left [ \\sum_{k=1}^{K}(\\hat{y}_k - y_k)w_{kj} \\right ] (1-h_j)h_{j}$  \n",
        "を，各$j$について計算しています。（この処理によって，`pref_grad`はサイズ$K$のベクトルからサイズ$J$のベクトルに更新されます。）  \n",
        "つまり  \n",
        "```\n",
        "prev_grad = np.zeros(J)\n",
        "for j in range(J):\n",
        "  prev_grad[j] = np.sum((y_hat - y_onehot)*W[:,j])*(1 - h[j])*h[j]\n",
        "```\n",
        "に相当します。  \n",
        "上の実装の`np.sum((y_hat - y_onehot)*W[:,j])`は$\\sum_{k=1}^{K}(\\hat{y}_k - y_k)w_{kj}$に対応していますが，これは行列形式でまとめて書くと，  \n",
        "$\\left[ \\begin{matrix} \\sum_{k=1}^{K}(\\hat{y}_k - y_k)w_{k1} & \\cdots &  \\sum_{k=1}^{K}(\\hat{y}_k - y_k)w_{kJ} \\end{matrix} \\right] =\n",
        "\\left[\\begin{matrix} \\hat{y}_{1} - y_{1} & \\cdots & \\hat{y}_{K} - y_{K}\\end{matrix} \\right]\n",
        "\\left[\\begin{matrix} w_{11} & \\cdots & w_{1J} \\\\ & \\vdots &  \\\\ w_{K1} & \\cdots & w_{KJ} \\end{matrix} \\right]\n",
        "$  \n",
        "となります。  \n",
        "いま，`prev_grad`には$\\left[\\begin{matrix} \\hat{y}_{1} - y_{1} & \\cdots & \\hat{y}_{K} - y_{K}\\end{matrix} \\right]$が格納されていますので，`np.dot(prev_grad, W)`としてやることで，上記の計算が行えます。あとは`(1 - h)*h`との掛け算を追加するだけです。  \n",
        "このようにして`prev_grad`を更新した後，`grad_W_mid`の更新は`grad_W`の更新と同じ要領で行えるわけです。\n"
      ],
      "metadata": {
        "id": "DREdjNHUoveT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0idvxBemqOeU"
      },
      "source": [
        "各関数が実装できれば，後は`10_03_neural_network.ipynb`とほとんど同じです。  \n",
        "\n",
        "各パラメータの初期値を決め，学習を実行します。  \n",
        "まずは，ノード数を2とした場合です。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YmZ596h0-lt"
      },
      "outputs": [],
      "source": [
        "# 中間層のノード数\n",
        "num_middle_node = 2\n",
        "# 学習率\n",
        "lr = 0.2\n",
        "# エポック数\n",
        "num_epochs = 30\n",
        "\n",
        "np.random.seed(0)\n",
        "# 中間層の初期値\n",
        "W_mid =  np.random.randn(num_middle_node, num_dimensions)\n",
        "b_mid = np.zeros(num_middle_node)\n",
        "# 出力層の初期値\n",
        "W =  np.random.randn(num_classes, num_middle_node)\n",
        "b = np.zeros(num_classes)\n",
        "\n",
        "\n",
        "# 損失関数の履歴\n",
        "loss_history = np.array([])\n",
        "# 正解率の履歴\n",
        "acc_history = np.array([])\n",
        "\n",
        "np.random.seed(0)\n",
        "for epoch in range(num_epochs):\n",
        "  # データをシャッフルしなおす。\n",
        "  # (局所解に陥りにくくなるため)\n",
        "  shuffle_index = np.random.permutation(np.arange(num_samples))\n",
        "  X_tmp = X[(shuffle_index)]\n",
        "  Y_tmp = Y[(shuffle_index)]\n",
        "  Y_onehot_tmp = Y_onehot[(shuffle_index)]\n",
        "  \n",
        "  loss = 0\n",
        "  acc = 0\n",
        "  for n in range(num_samples):\n",
        "    x = X_tmp[n,:]\n",
        "    y_onehot = Y_onehot_tmp[n]\n",
        "\n",
        "    # ニューラルネットワークに与える\n",
        "    y_hat, h = neural_network(x, W_mid, b_mid, W, b)\n",
        "  \n",
        "    # 勾配の計算\n",
        "    grad_W_mid, grad_b_mid, grad_W, grad_b = calc_gradient(y_hat, y_onehot, h, W, x)\n",
        "\n",
        "    # 更新\n",
        "    W -= lr * grad_W\n",
        "    b -= lr * grad_b\n",
        "    W_mid -= lr * grad_W_mid\n",
        "    b_mid -= lr * grad_b_mid\n",
        "\n",
        "    # 損失関数の蓄積\n",
        "    loss += cross_entropy(y_hat, y_onehot)\n",
        "  \n",
        "  # このエポックにおける識別正解率を算出\n",
        "  for n in range(num_samples):\n",
        "    x = X_tmp[n,:]\n",
        "    y = Y_tmp[n]\n",
        "    # ニューラルネットワークに与える\n",
        "    y_hat, h = neural_network(x, W_mid, b_mid, W, b)\n",
        "    # 正解率の蓄積\n",
        "    if np.argmax(y_hat) == y:\n",
        "      acc += 1\n",
        "  \n",
        "  loss /= num_samples\n",
        "  acc /= num_samples\n",
        "  loss_history = np.append(loss_history, loss)\n",
        "  acc_history = np.append(acc_history, acc*100)\n",
        "  print('%d-th epoch: cross entropy = %.3f, accuracy = %.3f%%' % (epoch+1, loss, acc*100))\n",
        "\n",
        "# クロスエントロピーおよび正解率のプロット\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss (cross entropy)')\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(acc_history)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy [%]')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je4-8YEccEJ4"
      },
      "source": [
        "ノード数が2の場合は，100%の正解率は得られませんでした。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvpCbnN4nzwu"
      },
      "source": [
        "識別境界を可視化します。  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gjbinbByfQz"
      },
      "outputs": [],
      "source": [
        "# x1，x2ごとの格子点の数\n",
        "grid_size = 200\n",
        "\n",
        "# 格子点：-3.1から3.1まで200個分，等間隔に区切った数列\n",
        "grid = np.linspace(-3.1, 3.1, grid_size)\n",
        "\n",
        "# クラス識別結果を色塗りする領域：200x200の図\n",
        "# 最後の3は，色情報(RGB=赤・緑・青の3次元ごとの色の強さ)\n",
        "image = np.zeros([grid_size, grid_size, 3])\n",
        "\n",
        "color_list = [(0, 0, 1), (1, 0, 0)]\n",
        "\n",
        "for i in range(grid_size):\n",
        "  for j in range(grid_size):\n",
        "    # 格子点のデータ\n",
        "    x = np.array([grid[j], grid[i]])\n",
        "    # 格子点データを識別する\n",
        "    y_hat,_ = neural_network(x, W_mid, b_mid, W, b)\n",
        "    label = np.argmax(y_hat)\n",
        "    # 識別結果のクラスの色で，色を塗る\n",
        "    # (グラフは原点が左下だが，画像は左上が原点なので，上下反転させるために-iとしている)\n",
        "    image[-i, j, :] = color_list[label]\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "# 色塗り結果を表示\n",
        "plt.imshow(image, alpha=0.4, extent=(-3.1, 3.1, -3.1, 3.1))\n",
        "# データおよび中間層の識別補助線を表示\n",
        "x1 = np.linspace(-3, 3)\n",
        "for j in range(num_middle_node):\n",
        "  x2 = -1.0 * (b_mid[j] + W_mid[j, 0]*x1) / W_mid[j, 1]\n",
        "  plt.plot(x1, x2, color='k')\n",
        "for k in range(num_classes):\n",
        "  plt.scatter(X[Y==k,0], X[Y==k,1], color=color_list[k], label='class'+str(k))\n",
        "plt.legend()\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.xlim([-3, 3])\n",
        "plt.ylim([-3, 3])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgNY7XAUtF6Q"
      },
      "source": [
        "ノード数が2の場合，識別境界を決める補助線の数が2本ということになります。  \n",
        "これでは，2クラスを100%分離する境界は作成できません。  \n",
        "\n",
        "ノード数が3の場合を実行してみます。  \n",
        "(ソースコードは上のコピペです。)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 中間層のノード数\n",
        "num_middle_node = 3\n",
        "# 学習率\n",
        "lr = 0.2\n",
        "# エポック数\n",
        "num_epochs = 30\n",
        "\n",
        "np.random.seed(0)\n",
        "# 中間層の初期値\n",
        "W_mid =  np.random.randn(num_middle_node, num_dimensions)\n",
        "b_mid = np.zeros(num_middle_node)\n",
        "# 出力層の初期値\n",
        "W =  np.random.randn(num_classes, num_middle_node)\n",
        "b = np.zeros(num_classes)\n",
        "\n",
        "\n",
        "# 損失関数の履歴\n",
        "loss_history = np.array([])\n",
        "# 正解率の履歴\n",
        "acc_history = np.array([])\n",
        "\n",
        "np.random.seed(0)\n",
        "for epoch in range(num_epochs):\n",
        "  # データをシャッフルしなおす。\n",
        "  # (局所解に陥りにくくなるため)\n",
        "  shuffle_index = np.random.permutation(np.arange(num_samples))\n",
        "  X_tmp = X[(shuffle_index)]\n",
        "  Y_tmp = Y[(shuffle_index)]\n",
        "  Y_onehot_tmp = Y_onehot[(shuffle_index)]\n",
        "  \n",
        "  loss = 0\n",
        "  acc = 0\n",
        "  for n in range(num_samples):\n",
        "    x = X_tmp[n,:]\n",
        "    y_onehot = Y_onehot_tmp[n]\n",
        "\n",
        "    # ニューラルネットワークに与える\n",
        "    y_hat, h = neural_network(x, W_mid, b_mid, W, b)\n",
        "  \n",
        "    # 勾配の計算\n",
        "    grad_W_mid, grad_b_mid, grad_W, grad_b = calc_gradient(y_hat, y_onehot, h, W, x)\n",
        "\n",
        "    # 更新\n",
        "    W -= lr * grad_W\n",
        "    b -= lr * grad_b\n",
        "    W_mid -= lr * grad_W_mid\n",
        "    b_mid -= lr * grad_b_mid\n",
        "\n",
        "    # 損失関数の蓄積\n",
        "    loss += cross_entropy(y_hat, y_onehot)\n",
        "  \n",
        "  # このエポックにおける識別正解率を算出\n",
        "  for n in range(num_samples):\n",
        "    x = X_tmp[n,:]\n",
        "    y = Y_tmp[n]\n",
        "    # ニューラルネットワークに与える\n",
        "    y_hat, h = neural_network(x, W_mid, b_mid, W, b)\n",
        "    # 正解率の蓄積\n",
        "    if np.argmax(y_hat) == y:\n",
        "      acc += 1\n",
        "  \n",
        "  loss /= num_samples\n",
        "  acc /= num_samples\n",
        "  loss_history = np.append(loss_history, loss)\n",
        "  acc_history = np.append(acc_history, acc*100)\n",
        "  print('%d-th epoch: cross entropy = %.3f, accuracy = %.3f%%' % (epoch+1, loss, acc*100))\n",
        "\n",
        "# クロスエントロピーおよび正解率のプロット\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss (cross entropy)')\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(acc_history)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy [%]')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uA4VuDnA6txy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "今度は100%の正解率が得られました。  \n",
        "続いて識別境界を可視化します。  "
      ],
      "metadata": {
        "id": "r-Xplsyn6znK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x1，x2ごとの格子点の数\n",
        "grid_size = 200\n",
        "\n",
        "# 格子点：-3.1から3.1まで200個分，等間隔に区切った数列\n",
        "grid = np.linspace(-3.1, 3.1, grid_size)\n",
        "\n",
        "# クラス識別結果を色塗りする領域：200x200の図\n",
        "# 最後の3は，色情報(RGB=赤・緑・青の3次元ごとの色の強さ)\n",
        "image = np.zeros([grid_size, grid_size, 3])\n",
        "\n",
        "color_list = [(0, 0, 1), (1, 0, 0)]\n",
        "\n",
        "for i in range(grid_size):\n",
        "  for j in range(grid_size):\n",
        "    # 格子点のデータ\n",
        "    x = np.array([grid[j], grid[i]])\n",
        "    # 格子点データを識別する\n",
        "    y_hat,_ = neural_network(x, W_mid, b_mid, W, b)\n",
        "    label = np.argmax(y_hat)\n",
        "    # 識別結果のクラスの色で，色を塗る\n",
        "    # (グラフは原点が左下だが，画像は左上が原点なので，上下反転させるために-iとしている)\n",
        "    image[-i, j, :] = color_list[label]\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "# 色塗り結果を表示\n",
        "plt.imshow(image, alpha=0.4, extent=(-3.1, 3.1, -3.1, 3.1))\n",
        "# データおよび中間層の識別補助線を表示\n",
        "x1 = np.linspace(-3, 3)\n",
        "for j in range(num_middle_node):\n",
        "  x2 = -1.0 * (b_mid[j] + W_mid[j, 0]*x1) / W_mid[j, 1]\n",
        "  plt.plot(x1, x2, color='k')\n",
        "for k in range(num_classes):\n",
        "  plt.scatter(X[Y==k,0], X[Y==k,1], color=color_list[k], label='class'+str(k))\n",
        "plt.legend()\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.xlim([-3, 3])\n",
        "plt.ylim([-3, 3])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HcbP7Kxa6wqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ノード数が3以上であれば，100%分離可能な境界が作成できていることが分かります。"
      ],
      "metadata": {
        "id": "QGkBMXGU66Sg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 実装について補足\n",
        "for ループを使わずに，難しい表現を使って1行にまとめて書いた理由は，ソースコードを短くしたかったわけでも，格好よく書きたかったわけでもありません。  \n",
        "pythonは行列計算が得意な反面，C言語と比べて，**forループの処理が遅い**という欠点があるためです。この欠点は，大規模なデータを使って繰り返し学習を行う処理においては致命的になります。  \n",
        "そのため，いかにベクトル・行列での計算を利用してforループを無くすかが重要になってきます。  \n",
        "ただ，forループを使わずに書く場合はバグを生みやすくなります。  \n",
        "ですので，forループを使った場合と比較して，計算結果が同じになることを確認しながら慎重に実装することが鉄則です。（私がこのプログラムを作る場合もそうしています。）"
      ],
      "metadata": {
        "id": "Kmc_uJl10-EM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "n6n68GaG7DqH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "11_01_neural_network.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}