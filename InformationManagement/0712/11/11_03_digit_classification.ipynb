{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbqkmaY9RA-z"
      },
      "source": [
        "# 第11回 その3: ニューラルネットワークを使った手書き数字認識\n",
        "ここでは，もう少し具体的なデータとして，手書き数字の識別を行います。  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zJI0HyoWGGc"
      },
      "source": [
        "## ステップ1: 手書き数字データの準備\n",
        "まずは必要ライブラリをインポートします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYfdOuD_WNd_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXb_O89VWQQY"
      },
      "source": [
        "機械学習のscikit-learnライブラリには，いくつかサンプルデータが用意されています。  \n",
        "ここでは，手書き数字データを使います。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scikit-learn ライブラリの手書き数字データのインポート\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "# 手書き数字データを読み込む\n",
        "digit_data=load_digits()\n",
        "\n",
        "print(np.shape(digit_data.images))\n",
        "print(np.shape(digit_data.target))"
      ],
      "metadata": {
        "id": "PLUOh5bK6Vyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "データ数は1797です。  \n",
        "`digit_data.images`には各データの画像情報が格納されています。  \n",
        "画像サイズは$8 \\times 8$で，各点には画素の輝度値が格納されています。  \n",
        "`digit_data.target`にはクラス(どの数字を描いているのか)情報が格納されています。  \n",
        "例としてに，データ番号0～9までの`images`と`target`を表示してみます。\n"
      ],
      "metadata": {
        "id": "C860Sn6oVPVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7, 7))\n",
        "\n",
        "# 0番目のデータそのものを表示\n",
        "print('images[0]:')\n",
        "print(digit_data.images[0])\n",
        "\n",
        "# 0～9番目のデータを図として表示\n",
        "print('images:')\n",
        "for n in range(10):\n",
        "  plt.subplot(4, 3, n+1)\n",
        "  plt.imshow(digit_data.images[n], cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "# target を表示\n",
        "print('target:')\n",
        "print(digit_data.target[:10])"
      ],
      "metadata": {
        "id": "QXXVXizSVOdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "今回，データは画像なので2次元行列です。  \n",
        "一方ニューラルネットワークの入力はベクトルなので，そのままだとニューラルネットワークに入力ができません。  \n",
        "そこで，画像の各行を横に連結して，$8 \\times 8 = 64$次元のベクトルに変換します。  \n",
        "これは numpy の reshape 関数を使って行います。  \n",
        "また，データの標準化と one-hot ベクトルの作成も行います。"
      ],
      "metadata": {
        "id": "bgQ5A56zW67O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データのサンプル数と画像の高さ，幅を得る\n",
        "num_samples, height, width = np.shape(digit_data.images)\n",
        "\n",
        "# 画像の2次元行列をベクトル化した際の次元数を計算\n",
        "num_dimensions = height * width\n",
        "\n",
        "# クラス数は0～9の10クラス\n",
        "num_classes = np.max(digit_data.target) + 1\n",
        "\n",
        "print('Nunber of samples: ' + str(num_samples))\n",
        "print('Number of dimensions: ' + str(num_dimensions))\n",
        "print('Number of classes: ' + str(num_classes))\n",
        "\n",
        "# digit_dataの形を[1797, 8, 8] から [1797, 64]へ変えることで，\n",
        "# 各サンプルの8x8次元画像を64次元ベクトルに変形する。\n",
        "X = np.reshape(digit_data.images, [num_samples, num_dimensions])\n",
        "\n",
        "# 標準化も行っておく。\n",
        "X = (X - np.mean(X, axis=0)) / (np.std(X, axis=0)+1E-7)\n",
        "\n",
        "# クラス情報 Y\n",
        "Y = digit_data.target\n",
        "\n",
        "# one-hotベクトルの作成\n",
        "Y_onehot = np.zeros([num_samples, num_classes])\n",
        "for n in range(num_samples):\n",
        "  # クラス番号(y[n])に対応するインデクスを1にする\n",
        "  Y_onehot[n, Y[n]] = 1"
      ],
      "metadata": {
        "id": "HTcf2rWnZKOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "今までは単一のデータセットに対して学習・評価を行っていましたが，一般的なクラス識別タスクでは，データセットを学習データと評価データに分けて行います。  \n",
        "（一般には評価データを事前に知ることは不可能なので，評価データは学習には使わないということです。）  \n",
        "ここでは，1797サンプルのうち，300サンプルを評価データに使用して，残りの1497サンプルを学習データに使用することにします。"
      ],
      "metadata": {
        "id": "1TXffw_xb5K1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 先頭の300サンプルを評価データに使用\n",
        "X_test = X[:300]\n",
        "Y_test = Y[:300]\n",
        "Y_onehot_test = Y_onehot[:300]\n",
        "\n",
        "# 残りのサンプルを学習データに使用\n",
        "X_train = X[300:]\n",
        "Y_train = Y[300:]\n",
        "Y_onehot_train = Y_onehot[300:]\n",
        "\n",
        "num_samples_train = np.size(Y_train)\n",
        "num_samples_test = np.size(Y_test)\n",
        "\n",
        "print('Nunber of training samples: ' + str(num_samples_train))\n",
        "print('Nunber of test samples: ' + str(num_samples_test))"
      ],
      "metadata": {
        "id": "4CnavtUs8RQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以上でデータの準備は完了です。"
      ],
      "metadata": {
        "id": "_i9kG3GIdQGK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ステップ2: ニューラルネットワークによる手書き数字認識  \n",
        "用意したデータを使ってニューラルネットワークの学習と評価を行います。  \n",
        "\n",
        "まずはニューラルネットワークの実装です。"
      ],
      "metadata": {
        "id": "W7Avv4fVdSMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  '''\n",
        "      シグモイド関数\n",
        "  '''\n",
        "  return (1+np.exp(-1*z))**(-1)\n",
        "\n",
        "\n",
        "def softmax(z):\n",
        "  '''\n",
        "      ソフトマックス関数\n",
        "      z: 要素数=クラス数のベクトル\n",
        "      y_hat: クラス毎の確率。要素数=クラス数のベクトル\n",
        "  '''\n",
        "  z_norm = z - np.max(z) # np.exp(z)のオーバーフロー対策\n",
        "  return (np.exp(z_norm) / np.sum(np.exp(z_norm)))\n",
        "\n",
        "\n",
        "def cross_entropy(y_hat, y_onehot):\n",
        "  '''\n",
        "      クロスエントロピー損失を計算する。\n",
        "      y_hat: マルチクラス回帰によって推定された y\n",
        "      y_onehot: 正解のクラスの確率が1，それ以外の確率を0とするベクトル\n",
        "                (one-hotベクトル)\n",
        "  '''\n",
        "  return -1.0 * np.sum(y_onehot*np.log(y_hat))"
      ],
      "metadata": {
        "id": "sSYMPaGedjGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ニューラルネットワークの実装は `11_02_neural_network2.ipynb` のものと同じです。"
      ],
      "metadata": {
        "id": "pvzgWaXFdoAc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkdO-S8SuDPX"
      },
      "outputs": [],
      "source": [
        "def initialize(num_inp, num_out, num_mid, seed=0):\n",
        "  '''\n",
        "      初期設定\n",
        "      num_inp: 入力層のノード数(データの次元数)\n",
        "      num_out: 出力層のノード数(=識別したいクラス数)\n",
        "      num_mid: 各中間層のノード数。リスト型で定義\n",
        "               中間層が2層で，それぞれノード数が5, 3とする場合\n",
        "               num_mid = [5, 3]とする。\n",
        "      seed: 乱数のシード。デフォルト値は0\n",
        "  '''\n",
        "  # 中間層の数は num_mid の要素数\n",
        "  num_mid_layers = len(num_mid)\n",
        "  # トータルの層数 = 中間層の数 + 1 (出力層の分)\n",
        "  num_layers = num_mid_layers + 1\n",
        "\n",
        "  np.random.seed(seed) # 乱数シードの固定\n",
        "\n",
        "  # 中間層の重み行列 W の設定（各層の重み行列がリスト型で格納）\n",
        "  W = []\n",
        "  b = []\n",
        "  for layer in range(num_layers):\n",
        "    # まず，各層への入力ノード数と出力ノード数を確認\n",
        "    if layer == 0:\n",
        "      # 最初の中間層は入力データが入るため，入力ノード数=num_inp(データの次元数)\n",
        "      n_in = num_inp\n",
        "    else:\n",
        "      # それ以外の場合，入力ノード数=一つ前の中間層のノード数\n",
        "      n_in = num_mid[layer-1]\n",
        "    if layer == num_layers - 1:\n",
        "      # 出力層の場合，出力ノード数=num_out(クラス数)\n",
        "      n_out = num_out\n",
        "    else:\n",
        "      # それ以外の場合，出力ノード数=中間層のノード数\n",
        "      n_out = num_mid[layer]\n",
        "      \n",
        "    # W のサイズは[出力ノード数x入力ノード数]．乱数に基づいて初期値を作成\n",
        "    W_tmp = np.random.normal(0, 1/np.sqrt(n_in), (n_out, n_in))\n",
        "    # b のサイズは[出力ノード数]\n",
        "    b_tmp = np.zeros(n_out)\n",
        "\n",
        "    # 重み行列リストに追加\n",
        "    W.append(W_tmp)\n",
        "    b.append(b_tmp)\n",
        "\n",
        "  return W, b\n",
        "\n",
        "\n",
        "def forward(x, W, b):\n",
        "  '''\n",
        "      ニューラルネットワークにデータを通す(フォワード処理)\n",
        "      x: 入力データ(サイズ=num_inpのベクトル)\n",
        "      W: 各中間層および出力層の重み行列をリスト化したもの\n",
        "      b: 各中間層および出力層のバイアスベクトルをリスト化したもの\n",
        "  '''\n",
        "  # 層の数 = 重み行列リストの要素数\n",
        "  num_layers = len(W)\n",
        "\n",
        "  # 層に通す。\n",
        "  # このとき，各層における活性化関数に通す前と通した後の計算結果をそれぞれ\n",
        "  # g, h としてリスト形式で記録しておく\n",
        "  g = [] # 非線形関数(活性化関数)を通す前\n",
        "  h = [] # 活性化関数を通した後\n",
        "  for layer in range(num_layers):\n",
        "    if layer == 0:\n",
        "      # 最初の中間層では，入力層の値(=入力データ x)との内積を取る\n",
        "      g_tmp = np.dot(W[layer], x) + b[layer]\n",
        "    else:\n",
        "      # 以降の層では，一つ前の層の値(h[-1])との内積を取る\n",
        "      g_tmp = np.dot(W[layer], h[-1]) + b[layer]\n",
        "      \n",
        "    if layer == num_layers - 1:\n",
        "      # 出力層の場合はソフトマックス関数に通す\n",
        "        h_tmp = softmax(g_tmp)\n",
        "    else:\n",
        "      # それ以外の場合はシグモイド関数に通す\n",
        "        h_tmp = sigmoid(g_tmp)\n",
        "\n",
        "    # リストに追加\n",
        "    g.append(g_tmp)\n",
        "    h.append(h_tmp)\n",
        "\n",
        "  # g と h を返す。最終的な出力はh[-1]\n",
        "  return g, h\n",
        "\n",
        "\n",
        "def backward(x, y, W, h):\n",
        "  '''\n",
        "      勾配を計算する(バックワード処理)\n",
        "      x: 入力データ(サイズ=num_inpのベクトル)\n",
        "      y: 正解の出力データ(サイズ=num_outのベクトル)\n",
        "      W: 各中間層および出力層の重み行列をリスト化したもの\n",
        "      h: 各中間層および出力層の計算結果をリスト化したもの\n",
        "  '''\n",
        "  # 層の数 = 重み行列リストの要素数\n",
        "  num_layers = len(W)\n",
        "  \n",
        "  # 勾配を計算\n",
        "  grad_W = []\n",
        "  grad_b = []\n",
        "  # forのrange()の後ろの[::-1]は逆順の意味。つまりnum_layers-1～0へカウントダウン。\n",
        "  for layer in range(num_layers)[::-1]:\n",
        "    # まず，prev_grad = ∂L/∂g を求める。prev_gradは逆伝播される。\n",
        "    # prev_gradは出力層とそれ以外で計算が異なる。\n",
        "    if layer == num_layers - 1:\n",
        "      # 出力層の場合\n",
        "      prev_grad = h[-1] - y\n",
        "    else:\n",
        "      # それ以外の層の場合\n",
        "      prev_grad = np.dot(prev_grad, W[layer+1])*(1 - h[layer])*h[layer]\n",
        "\n",
        "    # prev_gradを使って勾配を計算する。\n",
        "    if layer == 0:\n",
        "      # 入力層の場合\n",
        "      grad_W_tmp = np.dot(np.array([prev_grad]).T, np.array([x]))\n",
        "    else:\n",
        "      # それ以外の場合\n",
        "      grad_W_tmp = np.dot(np.array([prev_grad]).T, np.array([h[layer-1]]))\n",
        "    grad_b_tmp = prev_grad\n",
        "\n",
        "    # リストの先頭に追加\n",
        "    grad_W.insert(0, grad_W_tmp)\n",
        "    grad_b.insert(0, grad_b_tmp)\n",
        "  \n",
        "  # grad_Wとgrad_bを返す\n",
        "  return grad_W, grad_b\n",
        "\n",
        "\n",
        "def update(lr, W, b, grad_W, grad_b):\n",
        "  '''\n",
        "      勾配降下法によるパラメータの更新\n",
        "      lr: 学習率\n",
        "      W, b: 各中間層および出力層の重み行列とバイアスベクトルをリスト化したもの\n",
        "      grad_W, grad_b，各層の勾配をリスト化したもの\n",
        "  '''\n",
        "  # 層の数 = 重み行列リストの要素数\n",
        "  num_layers = len(W)\n",
        "\n",
        "  for layer in range(num_layers):\n",
        "    W[layer] -= lr * grad_W[layer]\n",
        "    b[layer] -= lr * grad_b[layer]\n",
        "  \n",
        "  return W, b"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "さて，学習と評価を行います。  \n",
        "それぞれの処理に対して，学習データと評価データに分けて行っている点に注意してください。"
      ],
      "metadata": {
        "id": "9ST8qeGPdzh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "まずは中間層の数が1，ノード数が20の3層ニューラルネットワークでの評価です。"
      ],
      "metadata": {
        "id": "Zu9qwpUtk0rU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YmZ596h0-lt"
      },
      "outputs": [],
      "source": [
        "# 中間層のノード数\n",
        "num_middle_node = [20]\n",
        "\n",
        "# 学習率\n",
        "lr = 0.2\n",
        "# エポック数\n",
        "num_epochs = 100\n",
        "\n",
        "# 重み行列の初期化を行う\n",
        "W, b = initialize(num_dimensions, num_classes, num_middle_node)\n",
        "\n",
        "\n",
        "# 損失関数の履歴\n",
        "loss_history = np.array([])\n",
        "# 正解率の履歴\n",
        "acc_train_history = np.array([])\n",
        "acc_test_history = np.array([])\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  \n",
        "  # 学習データを用いてニューラルネットワークのパラメータを更新\n",
        "\n",
        "  # データをシャッフルしなおす。\n",
        "  # (局所解に陥りにくくなるため)\n",
        "  shuffle_index = np.random.permutation(np.arange(num_samples_train))\n",
        "  X_tmp = X_train[(shuffle_index)]\n",
        "  Y_onehot_tmp = Y_onehot_train[(shuffle_index)]\n",
        "  \n",
        "  loss = 0\n",
        "  acc_train = 0\n",
        "  acc_test = 0\n",
        "  for n in range(num_samples_train):\n",
        "    x = X_tmp[n]\n",
        "    y_onehot = Y_onehot_tmp[n]\n",
        "\n",
        "    # ニューラルネットワークに与える\n",
        "    g, h = forward(x, W, b)\n",
        "    # 最終出力はリストhの最後の要素である\n",
        "    y_hat = h[-1]\n",
        "  \n",
        "    # 勾配の計算\n",
        "    grad_W, grad_b = backward(x, y_onehot, W, h)\n",
        "\n",
        "    # 更新\n",
        "    W, b = update(lr, W, b, grad_W, grad_b)\n",
        "\n",
        "    # 損失関数の蓄積\n",
        "    loss += cross_entropy(y_hat, y_onehot)\n",
        "  \n",
        "  # エポック毎の識別正解率を測る。\n",
        "  # ここでは学習データ，評価データ両方に対して正解率を求めている。\n",
        "  # 学習データ\n",
        "  for n in range(num_samples_train):\n",
        "    x = X_train[n]\n",
        "    y = Y_train[n]\n",
        "\n",
        "    # ニューラルネットワークに与える\n",
        "    g, h = forward(x, W, b)\n",
        "    # 最終出力はリストhの最後の要素である\n",
        "    y_hat = h[-1]\n",
        "    # 正解率の蓄積\n",
        "    if np.argmax(y_hat) == y:\n",
        "      acc_train += 1\n",
        "  \n",
        "  # 評価データ\n",
        "  for n in range(num_samples_test):\n",
        "    x = X_test[n]\n",
        "    y = Y_test[n]\n",
        "\n",
        "    # ニューラルネットワークに与える\n",
        "    g, h = forward(x, W, b)\n",
        "    # 最終出力はリストhの最後の要素である\n",
        "    y_hat = h[-1]\n",
        "    # 正解率の蓄積\n",
        "    if np.argmax(y_hat) == y:\n",
        "      acc_test += 1\n",
        "  \n",
        "  loss /= num_samples_train\n",
        "  acc_train /= num_samples_train\n",
        "  acc_test /= num_samples_test\n",
        "  loss_history = np.append(loss_history, loss)\n",
        "  acc_train_history = np.append(acc_train_history, acc_train*100)\n",
        "  acc_test_history = np.append(acc_test_history, acc_test*100)\n",
        "  print('%d-th epoch: cross entropy = %.3f, train_accuracy = %.3f%%, test_accuracy = %.3f%%' % (epoch+1, loss, acc_train*100, acc_test*100))\n",
        "\n",
        "\n",
        "# クロスエントロピーおよび正解率のプロット\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss (cross entropy)')\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(acc_train_history, label='training data')\n",
        "plt.plot(acc_test_history, label='test data')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy [%]')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習データに対しては100%，評価データに対しては92%の正解率でした。  \n",
        "評価データは学習データに含まれないデータですので，一般的には学習データの正解率に比べて悪くなります。"
      ],
      "metadata": {
        "id": "7irtWtQUlN_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "次に，中間層の数が2，ノード数が20, 10の4層ニューラルネットワークの評価です。"
      ],
      "metadata": {
        "id": "Nu2uvyvVleR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 中間層のノード数\n",
        "num_middle_node = [20, 10]\n",
        "\n",
        "# 学習率\n",
        "lr = 0.2\n",
        "# エポック数\n",
        "num_epochs = 100\n",
        "\n",
        "# 重み行列の初期化を行う\n",
        "W, b = initialize(num_dimensions, num_classes, num_middle_node)\n",
        "\n",
        "\n",
        "# 損失関数の履歴\n",
        "loss_history = np.array([])\n",
        "# 正解率の履歴\n",
        "acc_train_history = np.array([])\n",
        "acc_test_history = np.array([])\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  \n",
        "  # 学習データを用いてニューラルネットワークのパラメータを更新\n",
        "\n",
        "  # データをシャッフルしなおす。\n",
        "  # (局所解に陥りにくくなるため)\n",
        "  shuffle_index = np.random.permutation(np.arange(num_samples_train))\n",
        "  X_tmp = X_train[(shuffle_index)]\n",
        "  Y_onehot_tmp = Y_onehot_train[(shuffle_index)]\n",
        "  \n",
        "  loss = 0\n",
        "  acc_train = 0\n",
        "  acc_test = 0\n",
        "  for n in range(num_samples_train):\n",
        "    x = X_tmp[n]\n",
        "    y_onehot = Y_onehot_tmp[n]\n",
        "\n",
        "    # ニューラルネットワークに与える\n",
        "    g, h = forward(x, W, b)\n",
        "    # 最終出力はリストhの最後の要素である\n",
        "    y_hat = h[-1]\n",
        "  \n",
        "    # 勾配の計算\n",
        "    grad_W, grad_b = backward(x, y_onehot, W, h)\n",
        "\n",
        "    # 更新\n",
        "    W, b = update(lr, W, b, grad_W, grad_b)\n",
        "\n",
        "    # 損失関数の蓄積\n",
        "    loss += cross_entropy(y_hat, y_onehot)\n",
        "  \n",
        "  # エポック毎の識別正解率を測る。\n",
        "  # ここでは学習データ，評価データ両方に対して正解率を求めている。\n",
        "  # 学習データ\n",
        "  for n in range(num_samples_train):\n",
        "    x = X_train[n]\n",
        "    y = Y_train[n]\n",
        "\n",
        "    # ニューラルネットワークに与える\n",
        "    g, h = forward(x, W, b)\n",
        "    # 最終出力はリストhの最後の要素である\n",
        "    y_hat = h[-1]\n",
        "    # 正解率の蓄積\n",
        "    if np.argmax(y_hat) == y:\n",
        "      acc_train += 1\n",
        "  \n",
        "  # 評価データ\n",
        "  for n in range(num_samples_test):\n",
        "    x = X_test[n]\n",
        "    y = Y_test[n]\n",
        "\n",
        "    # ニューラルネットワークに与える\n",
        "    g, h = forward(x, W, b)\n",
        "    # 最終出力はリストhの最後の要素である\n",
        "    y_hat = h[-1]\n",
        "    # 正解率の蓄積\n",
        "    if np.argmax(y_hat) == y:\n",
        "      acc_test += 1\n",
        "  \n",
        "  loss /= num_samples_train\n",
        "  acc_train /= num_samples_train\n",
        "  acc_test /= num_samples_test\n",
        "  loss_history = np.append(loss_history, loss)\n",
        "  acc_train_history = np.append(acc_train_history, acc_train*100)\n",
        "  acc_test_history = np.append(acc_test_history, acc_test*100)\n",
        "  print('%d-th epoch: cross entropy = %.3f, train_accuracy = %.3f%%, test_accuracy = %.3f%%' % (epoch+1, loss, acc_train*100, acc_test*100))\n",
        "\n",
        "\n",
        "# クロスエントロピーおよび正解率のプロット\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss (cross entropy)')\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(acc_train_history, label='training data')\n",
        "plt.plot(acc_test_history, label='test data')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy [%]')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qf7RNMzKiAwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習データの正解率が100%，評価データの正解率が93%と少し精度が上がりました。  \n",
        "（たまたまの可能性が高いですが…。）"
      ],
      "metadata": {
        "id": "Dj9bZbg2m_0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "最後に，中間層のノード数が20, 10, 2となる5層ニューラルネットワークを学習させ，ノード数2の中間層の値をプロットしてみます。  "
      ],
      "metadata": {
        "id": "_cv5eZOrnPHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 中間層のノード数\n",
        "num_middle_node = [20, 20, 2]\n",
        "\n",
        "# 学習率\n",
        "lr = 0.2\n",
        "# エポック数\n",
        "num_epochs = 100\n",
        "\n",
        "# 重み行列の初期化を行う\n",
        "W, b = initialize(num_dimensions, num_classes, num_middle_node)\n",
        "\n",
        "\n",
        "# 損失関数の履歴\n",
        "loss_history = np.array([])\n",
        "# 正解率の履歴\n",
        "acc_train_history = np.array([])\n",
        "acc_test_history = np.array([])\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  \n",
        "  # 学習データを用いてニューラルネットワークのパラメータを更新\n",
        "\n",
        "  # データをシャッフルしなおす。\n",
        "  # (局所解に陥りにくくなるため)\n",
        "  shuffle_index = np.random.permutation(np.arange(num_samples_train))\n",
        "  X_tmp = X_train[(shuffle_index)]\n",
        "  Y_onehot_tmp = Y_onehot_train[(shuffle_index)]\n",
        "  \n",
        "  loss = 0\n",
        "  acc_train = 0\n",
        "  acc_test = 0\n",
        "  for n in range(num_samples_train):\n",
        "    x = X_tmp[n]\n",
        "    y_onehot = Y_onehot_tmp[n]\n",
        "\n",
        "    # ニューラルネットワークに与える\n",
        "    g, h = forward(x, W, b)\n",
        "    # 最終出力はリストhの最後の要素である\n",
        "    y_hat = h[-1]\n",
        "  \n",
        "    # 勾配の計算\n",
        "    grad_W, grad_b = backward(x, y_onehot, W, h)\n",
        "\n",
        "    # 更新\n",
        "    W, b = update(lr, W, b, grad_W, grad_b)\n",
        "\n",
        "    # 損失関数の蓄積\n",
        "    loss += cross_entropy(y_hat, y_onehot)\n",
        "  \n",
        "  # エポック毎の識別正解率を測る。\n",
        "  # ここでは学習データ，評価データ両方に対して正解率を求めている。\n",
        "  # 学習データ\n",
        "  for n in range(num_samples_train):\n",
        "    x = X_train[n]\n",
        "    y = Y_train[n]\n",
        "\n",
        "    # ニューラルネットワークに与える\n",
        "    g, h = forward(x, W, b)\n",
        "    # 最終出力はリストhの最後の要素である\n",
        "    y_hat = h[-1]\n",
        "    # 正解率の蓄積\n",
        "    if np.argmax(y_hat) == y:\n",
        "      acc_train += 1\n",
        "  \n",
        "  # 評価データ\n",
        "  for n in range(num_samples_test):\n",
        "    x = X_test[n]\n",
        "    y = Y_test[n]\n",
        "\n",
        "    # ニューラルネットワークに与える\n",
        "    g, h = forward(x, W, b)\n",
        "    # 最終出力はリストhの最後の要素である\n",
        "    y_hat = h[-1]\n",
        "    # 正解率の蓄積\n",
        "    if np.argmax(y_hat) == y:\n",
        "      acc_test += 1\n",
        "  \n",
        "  loss /= num_samples_train\n",
        "  acc_train /= num_samples_train\n",
        "  acc_test /= num_samples_test\n",
        "  loss_history = np.append(loss_history, loss)\n",
        "  acc_train_history = np.append(acc_train_history, acc_train*100)\n",
        "  acc_test_history = np.append(acc_test_history, acc_test*100)\n",
        "  print('%d-th epoch: cross entropy = %.3f, train_accuracy = %.3f%%, test_accuracy = %.3f%%' % (epoch+1, loss, acc_train*100, acc_test*100))\n",
        "\n",
        "\n",
        "# クロスエントロピーおよび正解率のプロット\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss (cross entropy)')\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(acc_train_history, label='training data')\n",
        "plt.plot(acc_test_history, label='test data')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy [%]')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lULIl51GnLp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "元々$8 \\times 8$の10クラスのデータの情報を，中間層で2次元に圧縮しているため，どうしても正解率は下がってしまいますが，一応学習は行えましたので，2次元のデータを可視化してみます。"
      ],
      "metadata": {
        "id": "3wCdShLHnu5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 出力層の直前の層の g の値\n",
        "g_train = np.zeros([num_samples_train, 2])\n",
        "g_test = np.zeros([num_samples_test, 2])\n",
        "\n",
        "for n in range(num_samples_train):\n",
        "  x = X_train[n]\n",
        "  # ニューラルネットワークに与える\n",
        "  g, h = forward(x, W, b)\n",
        "  # 出力層の直前の層のインデクスは -2 \n",
        "  g_train[n,:] = g[-2]\n",
        "\n",
        "for n in range(num_samples_test):\n",
        "  x = X_test[n]\n",
        "  # ニューラルネットワークに与える\n",
        "  g, h = forward(x, W, b)\n",
        "  # 出力層の直前の層のインデクスは -2 \n",
        "  g_test[n,:] = g[-2]\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "color_list = ['red', 'blue', 'green', 'yellow', 'cyan', 'magenta', 'black', 'brown', 'orange', 'lightgreen']\n",
        "for k in range(num_classes):\n",
        "  plt.scatter(g_train[Y_train==k,0], g_train[Y_train==k,1], color=color_list[k], label=str(k))\n",
        "  plt.scatter(g_test[Y_test==k,0], g_test[Y_test==k,1], color=color_list[k], marker='x')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XxXJqL23m_vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "●印が学習データ，x印がテストデータです。  \n",
        "このようにすると，「2」と「3」の分布が近いことや，「8」と「9」の分布が近いこと，各評価データがどの数字に間違えやすいかといったことが分かります。"
      ],
      "metadata": {
        "id": "HBRQPWXboIJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HLfJ2XK3hqdp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "11_03_digit_classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}