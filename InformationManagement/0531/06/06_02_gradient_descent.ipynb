{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_02_gradient_descent.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbqkmaY9RA-z"
      },
      "source": [
        "# 第6回 その2: 勾配降下法\n",
        "勾配降下法を使ったパラメータの最適化をデモンストレーションします。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "まずはライブラリのインポート。"
      ],
      "metadata": {
        "id": "nxULlDg9aJ7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "SYfdOuD_WNd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ステップ1: 二次関数の最小値\n",
        "損失関数への入力パラメータを$x$（スカラー），損失関数を$L = (5 - x)^2$とします。  \n",
        "このとき，損失関数を最小化する$x$は$x = 5$ですが，これを勾配降下法によって求めてみましょう。  "
      ],
      "metadata": {
        "id": "_zJI0HyoWGGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下は損失関数を計算する関数です。"
      ],
      "metadata": {
        "id": "UYIMWIKfjnJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(x, a):\n",
        "  '''\n",
        "      x: 入力パラメータ\n",
        "      a: 二次関数のパラメータ\n",
        "  '''\n",
        "  L = (a - x)**2\n",
        "  return L"
      ],
      "metadata": {
        "id": "h1i4nThCjr6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "二次関数のパラメータを$a = 5$とします。  \n",
        "$x$の初期値を $-5$とし，このときの損失値を計算してみます。"
      ],
      "metadata": {
        "id": "9vUkdUbukRPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 損失関数のパラメータ\n",
        "a = 5\n",
        "\n",
        "# xの初期値\n",
        "x_initial = -5\n",
        "\n",
        "# 損失値\n",
        "L = loss_function(x_initial, a)\n",
        "print('loss = ' + str(L))\n",
        "\n",
        "# 損失関数をプロットする\n",
        "x_line = np.linspace(-10, 20) # -10から10に引かれたx軸\n",
        "L_line = loss_function(x_line, a)\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(x_line, L_line)\n",
        "plt.xlabel('parameter x')\n",
        "plt.ylabel('loss function L')\n",
        "# 現在の x と損失値をプロット\n",
        "plt.scatter(x_initial, L, color='r')\n",
        "plt.xlim([-10, 20])\n",
        "plt.ylim([-5, 200])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kL19ywiNiHO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "さて，このときの勾配を計算します。  \n",
        "$L = (a - x)^2$ の $x$ に対する勾配は  \n",
        "$\\frac{\\partial L}{\\partial x} = -2(a-x)$  \n",
        "です。"
      ],
      "metadata": {
        "id": "SXb_O89VWQQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_gradient(x, a):\n",
        "  grad = -2.0 * (a - x)\n",
        "  return grad"
      ],
      "metadata": {
        "id": "0m2n3e1el44Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "現在の$x$に対して勾配を計算します。  \n",
        "また，計算された勾配を元に，損失関数上に接線を引きます。"
      ],
      "metadata": {
        "id": "OI4i1u6RmKQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grad = calc_gradient(x_initial, a)\n",
        "print('gradient of L for x_initial = ' + str(grad))\n",
        "\n",
        "# 損失関数をプロットする\n",
        "x_line = np.linspace(-10, 20) # -10から10に引かれたx軸\n",
        "L_line = loss_function(x_line, a)\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(x_line, L_line)\n",
        "plt.xlabel('parameter x')\n",
        "plt.ylabel('loss function L')\n",
        "# 現在の x と損失値をプロット\n",
        "plt.scatter(x_initial, L, color='r')\n",
        "# 接線を引く\n",
        "tangent = grad * (x_line - x_initial) + L\n",
        "plt.plot(x_line, tangent, color='red')\n",
        "plt.xlim([-10, 20])\n",
        "plt.ylim([-5, 200])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "whbgJf2EmjCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "勾配は負の値になっていることが分かります。  \n",
        "さて，計算された勾配を元に，勾配降下法を使って$x$を更新します。  \n",
        "$x_{\\rm next} = x - \\mu \\frac{\\partial L}{\\partial x}$  \n",
        "ここでは，$\\mu = 0.1$ とします。"
      ],
      "metadata": {
        "id": "PABGDR2Xm69F"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkibCXvsRv-8"
      },
      "source": [
        "# 学習率(lr = learning rate)\n",
        "lr = 0.1\n",
        "\n",
        "# 勾配を使って w を更新\n",
        "x_next = x_initial - lr * grad\n",
        "\n",
        "print('x_next = ' + str(x_next))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "勾配は負の値なので，$x$は正の方向へ修正されます。  \n",
        "その結果，$x = -3.0$となりました。  \n",
        "このときも同様に損失値と接線を求めてみます。  "
      ],
      "metadata": {
        "id": "6pg6Px41oQj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 損失値\n",
        "L_next = loss_function(x_next, a)\n",
        "print('loss = ' + str(L))\n",
        "\n",
        "# 勾配\n",
        "grad_next = calc_gradient(x_next, a)\n",
        "print('gradient of L for x_initial = ' + str(grad))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "# 損失関数をプロット\n",
        "plt.plot(x_line, L_line)\n",
        "# 一つ前の x と損失値\n",
        "plt.scatter(x_initial, L, color='r', label='previous x')\n",
        "# 一つ前の接線\n",
        "tangent = grad * (x_line - x_initial) + L\n",
        "plt.plot(x_line, tangent, color='r')\n",
        "# 更新後の x と損失値と接線\n",
        "plt.scatter(x_next, L_next, color='g', label='next x')\n",
        "tangent_next = grad_next * (x_line - x_next) + L_next\n",
        "plt.plot(x_line, tangent_next, color='g')\n",
        "plt.xlabel('parameter x')\n",
        "plt.ylabel('loss function L')\n",
        "plt.xlim([-10, 20])\n",
        "plt.ylim([-5, 200])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3XOQS6oPogTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "更新によって$x$は最適値に近づき，それに伴って損失値が小さくなり，また勾配も0に近づきました。  "
      ],
      "metadata": {
        "id": "A2xHCAEBWdLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以降，上記の処理を繰り返します。"
      ],
      "metadata": {
        "id": "Wdt9OdMNpvqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import animation, rc\n",
        "from IPython.display import HTML\n",
        "\n",
        "# 学習率(lr = learning rate)\n",
        "lr = 0.1\n",
        "\n",
        "# 更新回数\n",
        "num_iterations = 30\n",
        "\n",
        "# 初期値をコピー\n",
        "x = x_initial\n",
        "\n",
        "# 描画\n",
        "fig = plt.figure(figsize=(6,6))\n",
        "plt.plot(x_line, L_line)\n",
        "images = []\n",
        "\n",
        "for n in range(num_iterations):\n",
        "    # 損失値を計算\n",
        "    L = loss_function(x, a)\n",
        "    # 勾配を計算\n",
        "    grad = calc_gradient(x, a)\n",
        "\n",
        "    print(\"%d-th iteration, x=%.3f, loss: %.3f, grad: %.3f\" % (n, x, L, grad))\n",
        "\n",
        "    # 更新前の状態を描画\n",
        "    tangent = grad * (x_line - x) + L\n",
        "    img = plt.plot(x_line, tangent, color='r')\n",
        "    img.append(plt.scatter(x, L, color='r'))\n",
        "    img.append(plt.text(-8, 180, 'iteration: '+str(n), size='x-large'))\n",
        "    images.append(img)\n",
        "    \n",
        "    # 更新\n",
        "    x = x - lr * grad\n",
        "\n",
        "plt.xlim([-10, 20])\n",
        "plt.ylim([-5, 200])\n",
        "plt.xlabel('parameter x')\n",
        "plt.ylabel('loss function L')\n",
        "\n",
        "# アニメーション作成\n",
        "anim = animation.ArtistAnimation(fig, images, interval=100)\n",
        "\n",
        "# Google Colaboratoryの場合必要\n",
        "rc('animation', html='jshtml')\n",
        "plt.close()\n",
        "display(anim)\n"
      ],
      "metadata": {
        "id": "2l_DJKdxWf0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "更新を繰り返すことで，$x$が徐々に$a=5$の値に近づき，それに伴って損失関数が最小値に，勾配が0に近づいていることが分かります。"
      ],
      "metadata": {
        "id": "zxlefHECS2L0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ステップ2: 学習率による挙動の違い\n",
        "学習率$\\mu$が小さいと，更新の度合いが小さくなるため，収束は遅くなります。  \n",
        "一方，学習率を大きくすると，更新の度合いは大きくなりますが，$x$が最適値の前後を行ったり来たりする，**振動状態**が起こる場合があります。  \n",
        "ためしに学習率を0.9にして，上の処理を再実行してみます。"
      ],
      "metadata": {
        "id": "kxl2IKgJWiM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習率(lr = learning rate)\n",
        "lr = 0.9\n",
        "\n",
        "# 更新回数\n",
        "num_iterations = 30\n",
        "\n",
        "# 初期値をコピー\n",
        "x = x_initial\n",
        "\n",
        "# 描画\n",
        "fig = plt.figure(figsize=(6,6))\n",
        "plt.plot(x_line, L_line)\n",
        "images = []\n",
        "\n",
        "for n in range(num_iterations):\n",
        "    # 損失値を計算\n",
        "    L = loss_function(x, a)\n",
        "    # 勾配を計算\n",
        "    grad = calc_gradient(x, a)\n",
        "\n",
        "    print(\"%d-th iteration, x=%.3f, loss: %.3f, grad: %.3f\" % (n, x, L, grad))\n",
        "\n",
        "    # 更新前の状態を描画\n",
        "    tangent = grad * (x_line - x) + L\n",
        "    img = plt.plot(x_line, tangent, color='r')\n",
        "    img.append(plt.scatter(x, L, color='r'))\n",
        "    img.append(plt.text(-8, 180, 'iteration: '+str(n), size='x-large'))\n",
        "    images.append(img)\n",
        "    \n",
        "    # 更新\n",
        "    x = x - lr * grad\n",
        "\n",
        "plt.xlim([-10, 20])\n",
        "plt.ylim([-5, 200])\n",
        "plt.xlabel('parameter x')\n",
        "plt.ylabel('loss function L')\n",
        "\n",
        "# アニメーション作成\n",
        "anim = animation.ArtistAnimation(fig, images, interval=100)\n",
        "\n",
        "# Google Colaboratoryの場合必要\n",
        "rc('animation', html='jshtml')\n",
        "plt.close()\n",
        "display(anim)\n"
      ],
      "metadata": {
        "id": "VqR6OYUbUDz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$x$が5の前後で振動しており，収束が遅くなっていることが確認できます。  \n",
        "ちなみに，上の例において学習率を1.0以上にすると，振動が大きくなりすぎて収束しなくなります。  \n",
        "（lrを1.0以上にして，挙動を確認してみてください。）  \n",
        "ですので，上記の例においては，学習率はある程度小さい方が良いということになります。  "
      ],
      "metadata": {
        "id": "0idvxBemqOeU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ステップ3: 局所最適解の問題  \n",
        "上の例のように損失関数が二次関数の場合，うまく学習率を設定しておけば最適解にたどり着くことができます。  \n",
        "しかし極小値が複数あるような関数の場合，最適解にたどり着けないことがあります。  \n",
        "\n",
        "例として，以下のような損失関数を考えてみます。\n"
      ],
      "metadata": {
        "id": "D0ALihNaU-Vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function2(x):\n",
        "  '''\n",
        "      x: 入力パラメータ\n",
        "      a: 二次関数のパラメータ\n",
        "  '''\n",
        "  L = np.sin(x) + 0.05*(5 -x)**2\n",
        "  return L"
      ],
      "metadata": {
        "id": "iON3maL-o9AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "この式の勾配は以下のように定義されます。  "
      ],
      "metadata": {
        "id": "Xr7GzwhmY4dF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_gradient2(x):\n",
        "  grad = np.cos(x) - 0.10*(5 -x)\n",
        "  return grad"
      ],
      "metadata": {
        "id": "QsmNRhASY8WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$x$の初期値を$x = -10$として，プロットしてみます。"
      ],
      "metadata": {
        "id": "ejTbuoXWYBYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# xの初期値\n",
        "x_initial = -10\n",
        "\n",
        "# 損失値\n",
        "L = loss_function2(x_initial)\n",
        "print('loss = ' + str(L))\n",
        "\n",
        "# 損失関数をプロットする\n",
        "x_line = np.linspace(-20, 22) # -10から10に引かれたx軸\n",
        "L_line = loss_function2(x_line)\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(x_line, L_line)\n",
        "plt.xlabel('parameter x')\n",
        "plt.ylabel('loss function L')\n",
        "# 現在の x と損失値をプロット\n",
        "plt.scatter(x_initial, L, color='r')\n",
        "plt.xlim([-12, 22])\n",
        "plt.ylim([-2.5, 17])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dFZyX2cdV1kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ではこの損失関数のもとで，学習率を1.0として勾配降下法を実施してみましょう。"
      ],
      "metadata": {
        "id": "NlmTZczwywVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習率(lr = learning rate)\n",
        "lr = 1.0\n",
        "\n",
        "# 更新回数\n",
        "num_iterations = 30\n",
        "\n",
        "# 初期値をコピー\n",
        "x = x_initial\n",
        "\n",
        "# 描画\n",
        "fig = plt.figure(figsize=(6,6))\n",
        "plt.plot(x_line, L_line)\n",
        "images = []\n",
        "\n",
        "for n in range(num_iterations):\n",
        "    # 損失値を計算\n",
        "    L = loss_function2(x)\n",
        "    # 勾配を計算\n",
        "    grad = calc_gradient2(x)\n",
        "\n",
        "    print(\"%d-th iteration, x=%.3f, loss: %.3f, grad: %.3f\" % (n, x, L, grad))\n",
        "\n",
        "    # 更新前の状態を描画\n",
        "    #img1 = plt.plot(x, L, c='r', marker='o', markersize=8)\n",
        "    tangent = grad * (x_line - x) + L\n",
        "    img = plt.plot(x_line, tangent, color='r')\n",
        "    img.append(plt.scatter(x, L, color='r'))\n",
        "    img.append(plt.text(-10, 16, 'iteration: '+str(n), size='x-large'))\n",
        "    images.append(img)\n",
        "    \n",
        "    # 更新\n",
        "    x = x - lr * grad\n",
        "\n",
        "plt.xlim([-12, 22])\n",
        "plt.ylim([-2.5, 17])\n",
        "plt.xlabel('parameter x')\n",
        "plt.ylabel('loss function L')\n",
        "\n",
        "# アニメーション作成\n",
        "anim = animation.ArtistAnimation(fig, images, interval=100)\n",
        "\n",
        "# Google Colaboratoryの場合必要\n",
        "rc('animation', html='jshtml')\n",
        "plt.close()\n",
        "display(anim)\n"
      ],
      "metadata": {
        "id": "lr0aQmsVYLCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "損失関数の極小値に引っかかってしまい，最小値に到達できていないことが分かります。  \n",
        "このように，複数存在する極小値のことを，<font color='red'>**局所最適解（局所解）**</font>と呼びます。  \n"
      ],
      "metadata": {
        "id": "Q10ro6XQRZEl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "一般に，学習率が小さいと局所最適解に陥りやすいです。  \n",
        "では学習率を大きくするとどうでしょうか？  \n",
        "学習率を3.0にして，再度実行してみましょう。"
      ],
      "metadata": {
        "id": "tIZlS6dPy4JM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習率(lr = learning rate)\n",
        "lr = 3.0\n",
        "\n",
        "# 更新回数\n",
        "num_iterations = 30\n",
        "\n",
        "# 初期値をコピー\n",
        "x = x_initial\n",
        "\n",
        "# 描画\n",
        "fig = plt.figure(figsize=(6,6))\n",
        "plt.plot(x_line, L_line)\n",
        "images = []\n",
        "\n",
        "for n in range(num_iterations):\n",
        "    # 損失値を計算\n",
        "    L = loss_function2(x)\n",
        "    # 勾配を計算\n",
        "    grad = calc_gradient2(x)\n",
        "\n",
        "    print(\"%d-th iteration, x=%.3f, loss: %.3f, grad: %.3f\" % (n, x, L, grad))\n",
        "\n",
        "    # 更新前の状態を描画\n",
        "    #img1 = plt.plot(x, L, c='r', marker='o', markersize=8)\n",
        "    tangent = grad * (x_line - x) + L\n",
        "    img = plt.plot(x_line, tangent, color='r')\n",
        "    img.append(plt.scatter(x, L, color='r'))\n",
        "    img.append(plt.text(-10, 16, 'iteration: '+str(n), size='x-large'))\n",
        "    images.append(img)\n",
        "    \n",
        "    # 更新\n",
        "    x = x - lr * grad\n",
        "\n",
        "plt.xlim([-12, 22])\n",
        "plt.ylim([-2.5, 17])\n",
        "plt.xlabel('parameter x')\n",
        "plt.ylabel('loss function L')\n",
        "\n",
        "# アニメーション作成\n",
        "anim = animation.ArtistAnimation(fig, images, interval=100)\n",
        "\n",
        "# Google Colaboratoryの場合必要\n",
        "rc('animation', html='jshtml')\n",
        "plt.close()\n",
        "display(anim)\n"
      ],
      "metadata": {
        "id": "4KKtTB4mX4Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "局所最適解に引っかかることはなくなりましたが，今度は振動状態が発生し，結局最適解にたどり着けませんでした。  \n",
        "このように，局所最適解は難しい問題となっています。"
      ],
      "metadata": {
        "id": "OViMDQCAZcUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Na0SIqbuzzCE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}