{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbqkmaY9RA-z"
      },
      "source": [
        "# 第10回 その3: ニューラルネットワークによる非線形識別\n",
        "いよいよニューラルネットワークを実装します。  \n",
        "ここでは中間層が一つ，中間層のノード数が2のニューラルネットワークを実装します。    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1DmeW6oRs8r"
      },
      "source": [
        "## ステップ0: Google Driveのマウントと作業フォルダへの移動  \n",
        "Google Drive に配置したデータを読み込むための準備です。  \n",
        "詳細については第二回の 02_01_graph.ipynb を参照してください。  \n",
        "\n",
        "ここでは\"マイドライブ/情報管理/10\"を作業フォルダとします。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4b4cfHPRXML"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# フォルダの移動には\"%cd\"を使用します。\n",
        "# 作業フォルダへ移動\n",
        "%cd /content/drive/'My Drive'/情報管理/10/\n",
        "# 現在のフォルダの中身を表示\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF6-hyJ2Sdau"
      },
      "source": [
        "`2class_data_nl.csv`というデータが表示されていることを確認してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zJI0HyoWGGc"
      },
      "source": [
        "## ステップ1: データの読み込みと標準化\n",
        "まずは必要ライブラリをインポートします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYfdOuD_WNd_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXb_O89VWQQY"
      },
      "source": [
        "`2class_data_nl.csv` を読み込みます。  \n",
        "このデータはx1とx2の二次元データで，クラス0とクラス1の2クラスのどちらかに属しています。  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkibCXvsRv-8"
      },
      "outputs": [],
      "source": [
        "# pandas の関数 read_csv を用いた csvファイル読み込み\n",
        "csv_data = pd.read_csv('2class_data_nl.csv', encoding='SHIFT-JIS')\n",
        "\n",
        "# データの前半部(.headで取得できる)のみ表示\n",
        "display(csv_data.head())\n",
        "\n",
        "# x1とx2のデータを抽出し，numpy用データ(ndarray型) に変換する。\n",
        "X = csv_data.loc[:, ['x1','x2']].to_numpy()\n",
        "\n",
        "# クラス番号を抽出し，numpy用データ(ndarray型) に変換する。\n",
        "Y = csv_data.loc[:,'class'].to_numpy()\n",
        "\n",
        "# データのサンプル数と次元数を得る。\n",
        "(num_samples, num_dimensions) = np.shape(X)\n",
        "print('Nunber of samples: ' + str(num_samples))\n",
        "print('Number of dimensions: ' + str(num_dimensions))\n",
        "\n",
        "# クラス数を得る。\n",
        "num_classes = int(np.max(Y) + 1)\n",
        "print('Number of classes: ' + str(num_classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2xHCAEBWdLt"
      },
      "source": [
        "標準化を行い，データをプロットします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2l_DJKdxWf0o"
      },
      "outputs": [],
      "source": [
        "# データを標準化する。\n",
        "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
        "\n",
        "# 以下は color_list = ['b', 'r'] とほぼ同じ。\n",
        "# 光の三原色(赤,緑,青)の値それぞれを0～1の範囲で定義している。\n",
        "color_list = [(0, 0, 1), (1, 0, 0)]\n",
        "\n",
        "# データをプロット\n",
        "plt.figure(figsize=(7,7))\n",
        "for k in range(num_classes):\n",
        "  # n番目のクラスに属している(Y==c)データをプロット\n",
        "  plt.scatter(X[Y==k,0], X[Y==k,1], color=color_list[k], label='class'+str(k))\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.legend()\n",
        "plt.xlim([-3, 3])\n",
        "plt.ylim([-3, 3])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "このデータ`2class_data_nl.csv`は**直線で2クラスに識別することはできません**。  \n",
        "つまり，<font color=red>**線形分離不可能なデータ**</font>ということになります。  \n",
        "このようなデータに対しては，ロジスティック回帰や`09_01_softmax_3class.ipynb`で実装した方法のような**線形モデル**では100%の精度で識別することができません。  \n",
        "そこでここでは**非線形モデル** (non-linear model) であるニューラルネットワークを使用して識別することにします。  \n"
      ],
      "metadata": {
        "id": "aqOZf7Gv58NU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxl2IKgJWiM-"
      },
      "source": [
        "## ステップ2: ニューラルネットワークの実装  \n",
        "ここでは，中間層の数を1，中間層のノードの数を2としたニューラルネットワークを実装します。  \n",
        "このモデルの式をは以下のようになります。  \n",
        "\n",
        "$\\left[\\begin{matrix}g_{1}\\\\g_{2} \\end{matrix} \\right] = \n",
        "\\left[\\begin{matrix} w_{11}^{\\rm mid}x_1 + w_{12}^{\\rm mid}x_2 + b_{1}^{\\rm mid}\\\\ w_{21}^{\\rm mid}x_1 + w_{22}^{\\rm mid}x_2 + b_{2}^{\\rm mid} \\end{matrix} \\right] = \n",
        "\\left[\\begin{matrix} w_{11}^{\\rm mid} & w_{12}^{\\rm mid} \\\\ w_{21}^{\\rm mid} & w_{22}^{\\rm mid} \\end{matrix} \\right]\\left[\\begin{matrix}x_1\\\\ x_2\\end{matrix} \\right] + \\left[\\begin{matrix}b_{1}^{\\rm mid}\\\\b_{2}^{\\rm mid} \\end{matrix} \\right] = {\\bf W}^{\\rm mid}{\\bf x} + {\\bf b}^{\\rm mid}$  \n",
        "$\\left[\\begin{matrix}h_{1}\\\\h_{2} \\end{matrix} \\right] = \n",
        "\\left[\\begin{matrix}{\\rm sigmoid}(g_1) \\\\ {\\rm sigmoid}(g_2)\\end{matrix} \\right]$  \n",
        "$\\left[\\begin{matrix}z_{1}\\\\z_{2} \\end{matrix} \\right] = \n",
        "\\left[\\begin{matrix} w_{11}h_1 + w_{12}h_2 + b_{1}\\\\ w_{21}h_1 + w_{22}h_2 + b_{2} \\end{matrix} \\right] = \n",
        "\\left[\\begin{matrix} w_{11} & w_{12} \\\\ w_{21} & w_{22} \\end{matrix} \\right]\\left[\\begin{matrix}h_1\\\\h_2 \\end{matrix} \\right] + \\left[\\begin{matrix}b_{1}\\\\ b_{2} \\end{matrix} \\right] = {\\bf W}{\\bf h} + {\\bf b}$  \n",
        "$\\left[\\begin{matrix}\\hat{y}_{1}\\\\\\hat{y}_{2} \\end{matrix} \\right] = \n",
        "\\left[\\begin{matrix}{\\rm softmax}(z_1) \\\\ {\\rm softmax}(z_2)\\end{matrix} \\right]$\n",
        "\n",
        "${\\bf W}^{\\rm mod}$は中間層における重み行列で，サイズは ノード数 $\\times$ 次元数 = $2 \\times 2$です。  \n",
        "${\\bf b}^{\\rm mod}$ は中間層における切片（バイアス）ベクトルで，サイズは ノード数$=2$ です。    \n",
        "${\\bf W}$は出力層における重み行列で，サイズは クラス数 $\\times$ 中間層のノード数 = $2 \\times 2$です。  \n",
        "${\\bf b}$ は出力層における切片（バイアス）ベクトルで，サイズは クラス数$=2$ です。  \n",
        "全部サイズが2ですが，それぞれ次元数，ノード数，クラス数と意味が違うので注意してください。（レポート課題で重要になるかもしれません。）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RmuM3aYnanP"
      },
      "source": [
        "1サンプルデータに対してニューラルネットワークを通し，$\\bf \\hat{y}$ を計算する関数を以下に定義します。  \n",
        "ちなみに，以下の neural_network 関数は中間層のノード数が2の場合でしか使えない（汚い）実装になっています。  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkdO-S8SuDPX"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "  '''\n",
        "      シグモイド関数\n",
        "  '''\n",
        "  return (1+np.exp(-1*z))**(-1)\n",
        "\n",
        "def softmax(z):\n",
        "  '''\n",
        "      ソフトマックス関数\n",
        "      z: 要素数=クラス数のベクトル\n",
        "      y_hat: クラス毎の確率。要素数=クラス数のベクトル\n",
        "  '''\n",
        "  return (np.exp(z) / np.sum(np.exp(z)))\n",
        "\n",
        "def neural_network(x, w_mid_1, b_mid_1, w_mid_2, b_mid_2, W, b):\n",
        "  '''\n",
        "      1サンプルデータに対してニューラルネットワークによるクラス分類を行う\n",
        "      x: 1サンプルデータ。サイズ=Dのベクトル\n",
        "      w_mid_1: 中間層のノード1に関する重み行列。サイズ=Dのベクトル\n",
        "      b_mid_1: 中間層のノード1に関するバイアス。スカラ\n",
        "      w_mid_2: 中間層のノード2に関する重み行列。サイズ=Dのベクトル\n",
        "      b_mid_2: 中間層のノード2に関するバイアス。スカラ\n",
        "      W: 出力層の重み行列。サイズ=(KxJ)の行列\n",
        "      b: 出力層のバイアス。サイズ=Kの行列\n",
        "      ただし，D:次元数，J:中間層のノード数，K:クラス数である。\n",
        "      y_hat: 出力層の出力。サイズ=Kのベクトル\n",
        "      h1: 中間層のノード1の出力。スカラ\n",
        "      h2: 中間層のノード2の出力。スカラ\n",
        "  '''\n",
        "  # 中間層を計算\n",
        "  # ノード1を計算\n",
        "  g1 = np.dot(w_mid_1, x) + b_mid_1\n",
        "  h1 = sigmoid(g1)\n",
        "  # ノード2を計算\n",
        "  g2 = np.dot(w_mid_2, x) + b_mid_2\n",
        "  h2 = sigmoid(g2)\n",
        "\n",
        "  # 出力層を計算\n",
        "  z = np.dot(W, np.array([h1, h2])) + b\n",
        "  y_hat = softmax(z)\n",
        "\n",
        "  return y_hat, h1, h2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "neural_network関数の`w_mid_1`は$\\left[\\begin{matrix} w_{11}^{\\rm mid} & w_{12}^{\\rm mid} \\end{matrix}\\right]$，`w_mid_2`は$\\left[\\begin{matrix} w_{21}^{\\rm mid} & w_{22}^{\\rm mid} \\end{matrix}\\right]$のことです。  \n",
        "`b_mid_1`，`b_mid_2`はそれぞれ$b_{1}^{\\rm mid}, b_{2}^{\\rm mid}$のことです。  \n",
        "`W`，`b`はそれぞれ$\\bf W$，$\\bf b$のことです。"
      ],
      "metadata": {
        "id": "Cha_GuHdAG7G"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0idvxBemqOeU"
      },
      "source": [
        "各パラメータの初期値を決めます。  \n",
        "ロジスティック回帰のときと同様，${\\bf b}^{\\rm mod}$ および ${\\bf b}$ の初期値はゼロ，${\\bf W}^{\\rm mod}$ および ${\\bf W}$ の初期値は正規分布に従う乱数で決定します。   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iON3maL-o9AV"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "# 中間層の初期値\n",
        "# ノード1\n",
        "initial_w_mid_1 =  np.random.randn(num_dimensions)\n",
        "initial_b_mid_1 = 0\n",
        "# ノード2\n",
        "initial_w_mid_2 =  np.random.randn(num_dimensions)\n",
        "initial_b_mid_2 = 0\n",
        "\n",
        "# 出力層の初期値\n",
        "initial_W =  np.random.randn(num_classes, 2) # ここの2はノード数の2に相当\n",
        "initial_b = np.zeros(num_classes)\n",
        "\n",
        "print('initial_w_mid_1 = ')\n",
        "print(initial_w_mid_1)\n",
        "print('initial_w_mid_2 = ')\n",
        "print(initial_w_mid_2)\n",
        "print('initial_W = ')\n",
        "print(initial_W)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu0ihSZ-tPcX"
      },
      "source": [
        "クロスエントロピー損失を計算する関数および勾配を計算する関数を定義します。  \n",
        "勾配の計算式は以下のように定義されます。  \n",
        "$\\frac{\\partial L_{ce}}{\\partial {\\bf W}_{k}} = (\\hat{y}_k - y_k){\\bf h}$  \n",
        "$\\frac{\\partial L_{ce}}{\\partial {\\bf b}_{k}} = \\hat{y}_k - y_k$  \n",
        "$\\frac{\\partial L_{ce}}{\\partial {\\bf W}_{j}^{\\rm mod}} = \\left [ \\sum_{k=1}^{K}(\\hat{y}_k - y_k)w_{kj} \\right ] (1-h_j)h_j{\\bf x}$   \n",
        "$\\frac{\\partial L_{ce}}{\\partial {\\bf b}_{j}^{\\rm mod}} = \\left [ \\sum_{k=1}^{K}(\\hat{y}_k - y_k)w_{kj} \\right ] (1-h_j)h_j$ \n",
        "\n",
        "$k$は出力層のノード番号（つまりクラス番号），$j$は中間層のノード番号です。   \n",
        "$y_k$は正解ラベルで，$k$が正解クラスの場合は確率1，不正解クラスの場合は0です。  \n",
        "\n",
        "なお，関数 calc_gradientも中間層のノード数が2の時にしか使えない（汚い）実装になっています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G33yRYprRsh"
      },
      "outputs": [],
      "source": [
        "def cross_entropy(y_hat, y):\n",
        "  '''\n",
        "      クロスエントロピー損失を計算する。\n",
        "      y_hat: マルチクラス回帰によって推定された y\n",
        "      y: 正解のクラス\n",
        "  '''\n",
        "  # 正解クラスの確率を1，それ以外のクラスの確率を0とするベクトル（one-hotベクトル）を作成する。\n",
        "  y_onehot = np.zeros(num_classes)\n",
        "  y_onehot[y] = 1.0\n",
        "\n",
        "  ce = -1.0 * np.sum(y_onehot*np.log(y_hat))\n",
        "\n",
        "  return ce\n",
        "\n",
        "def calc_gradient(y_hat, y, h1, h2, W, x):\n",
        "  '''\n",
        "      勾配を計算する\n",
        "      y_hat: 出力層の出力: サイズ=Kのベクトル\n",
        "      y: 正解のクラス番号: スカラ\n",
        "      h1: 中間層のノード1の出力。スカラ\n",
        "      h2: 中間層のノード2の出力。スカラ\n",
        "      W: 出力層の重み行列。サイズ=(KxJ)の行列\n",
        "      x: データ: サイズ=Dのベクトル\n",
        "      ただし，D:次元数，J:中間層のノード数，K:クラス数である。\n",
        "      grad_w_mid_1: w_mid_1の勾配：サイズ=Dのベクトル\n",
        "      grad_b_mid_1: b_mid_1の勾配：スカラ\n",
        "      grad_w_mid_2: w_mid_2の勾配：サイズ=Dのベクトル\n",
        "      grad_b_mid_2: b_mid_2の勾配：スカラ\n",
        "      grad_W: Wの勾配: サイズ=(KxJ)の行列\n",
        "      grad_b: bの勾配: サイズ=Kのベクトル\n",
        "  '''\n",
        "  # 次元数D, 中間層のノード数J, クラス数Kを得る。\n",
        "  D = np.size(x)\n",
        "  J = 2\n",
        "  K = np.size(y_hat)\n",
        "\n",
        "  # 正解クラスの確率を1，それ以外のクラスの確率を0とするベクトル（one-hotベクトル）を作成する。\n",
        "  y_onehot = np.zeros(K)\n",
        "  y_onehot[y] = 1.0\n",
        "\n",
        "  # 出力層の勾配を計算\n",
        "  grad_W = np.zeros([K, J])\n",
        "  grad_b = np.zeros(K)\n",
        "  for k in range(K):\n",
        "    grad_W[k,:] = (y_hat[k] - y_onehot[k]) * np.array([h1, h2])\n",
        "    grad_b[k] = y_hat[k] - y_onehot[k]\n",
        "\n",
        "  # 中間層の勾配を計算\n",
        "  # ノード1(W[:,0],h1を使っている点に注意)\n",
        "  grad_w_mid_1 = np.sum((y_hat - y_onehot)*W[:,0])*(1 - h1)*h1 * x\n",
        "  grad_b_mid_1 = np.sum((y_hat - y_onehot)*W[:,0])*(1 - h1)*h1\n",
        "  # ノード2(W[:,1],h2を使っている点に注意)\n",
        "  grad_w_mid_2 = np.sum((y_hat - y_onehot)*W[:,1])*(1 - h2)*h2 * x\n",
        "  grad_b_mid_2 = np.sum((y_hat - y_onehot)*W[:,1])*(1 - h2)*h2\n",
        "  \n",
        "  return grad_w_mid_1, grad_b_mid_1, grad_w_mid_2, grad_b_mid_2, grad_W, grad_b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKI-bcdJXlhT"
      },
      "source": [
        "ではロジスティック回帰のときと同様に，全てのデータを用いて繰り返し更新しましょう。   \n",
        "ここでは全データサンプルに対して更新を行うひとまとまりの処理を60周（エポック）行っています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YmZ596h0-lt"
      },
      "outputs": [],
      "source": [
        "from matplotlib import animation, rc\n",
        "from IPython.display import HTML\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "# 学習率\n",
        "lr = 0.2\n",
        "\n",
        "# パラメータの初期化\n",
        "w_mid_1 = initial_w_mid_1.copy()\n",
        "b_mid_1 = initial_b_mid_1\n",
        "w_mid_2 = initial_w_mid_2.copy()\n",
        "b_mid_2 = initial_b_mid_2\n",
        "W = initial_W.copy()\n",
        "b = initial_b.copy()\n",
        "\n",
        "# 描画\n",
        "color_list = [(0, 0, 1), (1, 0, 0)]\n",
        "fig = plt.figure(figsize=(7,7))\n",
        "# データの描画\n",
        "for k in range(num_classes):\n",
        "  plt.scatter(X[Y==k,0], X[Y==k,1], color=color_list[k], label='class'+str(k))\n",
        "images = []\n",
        "\n",
        "# 損失関数の履歴\n",
        "loss_history = np.array([])\n",
        "# 正解率の履歴\n",
        "acc_history = np.array([])\n",
        "\n",
        "for epoch in range(60):\n",
        "  \n",
        "  # データをシャッフルしなおす。\n",
        "  # (局所解に陥りにくくなるため)\n",
        "  shuffle_index = np.random.permutation(np.arange(num_samples))\n",
        "  X_tmp = X[(shuffle_index)]\n",
        "  Y_tmp = Y[(shuffle_index)]\n",
        "  \n",
        "  loss = 0\n",
        "  acc = 0\n",
        "  for n in range(num_samples):\n",
        "    x = X_tmp[n,:]\n",
        "    y = Y_tmp[n]\n",
        "  \n",
        "    # ニューラルネットワークに与える\n",
        "    y_hat, h1, h2 = neural_network(x, w_mid_1, b_mid_1, w_mid_2, b_mid_2, W, b)\n",
        "  \n",
        "    # 勾配の計算\n",
        "    grad_w_mid_1, grad_b_mid_1, grad_w_mid_2, grad_b_mid_2, grad_W, grad_b = calc_gradient(y_hat, y, h1, h2, W, x)\n",
        "\n",
        "    # 2エポック毎にアニメーション描画\n",
        "    if n == 0 and epoch % 2 == 0:\n",
        "      img = []\n",
        "      x1 = np.linspace(-3, 3)\n",
        "      x2 = -1.0 * (b_mid_1 + w_mid_1[0]*x1) / w_mid_1[1]\n",
        "      img += plt.plot(x1, x2, color='k')\n",
        "      x2 = -1.0 * (b_mid_2 + w_mid_2[0]*x1) / w_mid_2[1]\n",
        "      img += plt.plot(x1, x2, color='k')\n",
        "      img.append(plt.text(-2.8, 2.7, 'epoch: '+str(epoch), size='x-large'))\n",
        "      images.append(img)\n",
        "\n",
        "    # 更新\n",
        "    W -= lr * grad_W\n",
        "    b -= lr * grad_b\n",
        "    w_mid_1 -= lr * grad_w_mid_1\n",
        "    b_mid_1 -= lr * grad_b_mid_1\n",
        "    w_mid_2 -= lr * grad_w_mid_2\n",
        "    b_mid_2 -= lr * grad_b_mid_2\n",
        "\n",
        "    # 損失関数の蓄積\n",
        "    loss += cross_entropy(y_hat, y)\n",
        "    # 正解率の蓄積\n",
        "    if np.argmax(y_hat) == y:\n",
        "      acc += 1\n",
        "  \n",
        "  loss /= num_samples\n",
        "  acc /= num_samples\n",
        "  loss_history = np.append(loss_history, loss)\n",
        "  acc_history = np.append(acc_history, acc*100)\n",
        "  print('%d-th epoch: cross entropy = %.3f, accuracy = %.3f%%' % (epoch+1, loss, acc*100))\n",
        "\n",
        "plt.legend()\n",
        "plt.xlim([-3, 3])\n",
        "plt.ylim([-3, 3])\n",
        "\n",
        "# アニメーション作成\n",
        "anim = animation.ArtistAnimation(fig, images, interval=200)\n",
        "# Google Colaboratoryの場合必要\n",
        "rc('animation', html='jshtml')\n",
        "plt.close()\n",
        "display(anim)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記のアニメ－ションでは，中間層の2個のノードそれぞれに関する直線をプロットしています。  "
      ],
      "metadata": {
        "id": "DHK_KmLRGjl_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je4-8YEccEJ4"
      },
      "source": [
        "エポック毎の損失関数（クロスエントロピー）および分類正解率をプロットします。  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIjzgawo2NbZ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss (cross entropy)')\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(acc_history)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy [%]')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OL88EjpdRZK"
      },
      "source": [
        "（エポック毎に計算していますが）学習完了後の分類正解率を計算します。  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KgbvmdvcwPI"
      },
      "outputs": [],
      "source": [
        "# 正解率\n",
        "accuracy = 0\n",
        "\n",
        "for n in range(num_samples):\n",
        "  x = X[n,:]\n",
        "  y = Y[n]\n",
        "  # ニューラルネットワークに与える\n",
        "  y_hat, h1, h2 = neural_network(x, w_mid_1, b_mid_1, w_mid_2, b_mid_2, W, b)\n",
        "\n",
        "  if np.argmax(y_hat) == y:\n",
        "    accuracy += 1\n",
        "\n",
        "accuracy = 100.0 * accuracy / num_samples\n",
        "\n",
        "print('Accuracy = %.3f' % (accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RztiWfSZeRPZ"
      },
      "source": [
        "100%の正解率が得られました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvpCbnN4nzwu"
      },
      "source": [
        "最後に，識別境界を可視化します。  \n",
        "今回，中間層には2個のノードを設定しているので，中間層においては2本の直線が学習されていることになります。  \n",
        "ここでは，識別境界に加えて，中間層で学習された2本の直線もプロットしてみます。  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gjbinbByfQz"
      },
      "outputs": [],
      "source": [
        "# x1，x2ごとの格子点の数\n",
        "grid_size = 200\n",
        "\n",
        "# 格子点：-3.1から3.1まで200個分，等間隔に区切った数列\n",
        "grid = np.linspace(-3.1, 3.1, grid_size)\n",
        "\n",
        "# クラス識別結果を色塗りする領域：200x200の図\n",
        "# 最後の3は，色情報(RGB=赤・緑・青の3次元ごとの色の強さ)\n",
        "image = np.zeros([grid_size, grid_size, 3])\n",
        "\n",
        "color_list = [(0, 0, 1), (1, 0, 0)]\n",
        "\n",
        "for i in range(grid_size):\n",
        "  for j in range(grid_size):\n",
        "    # 格子点のデータ\n",
        "    x = np.array([grid[j], grid[i]])\n",
        "    # 格子点データを識別する\n",
        "    y_hat, h1, h2 = neural_network(x, w_mid_1, b_mid_1, w_mid_2, b_mid_2, W, b)\n",
        "    label = np.argmax(y_hat)\n",
        "    # 識別結果のクラスの色で，色を塗る\n",
        "    # (グラフは原点が左下だが，画像は左上が原点なので，上下反転させるために-iとしている)\n",
        "    image[-i, j, :] = color_list[label]\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "# 色塗り結果を表示\n",
        "plt.imshow(image, alpha=0.4, extent=(-3.1, 3.1, -3.1, 3.1))\n",
        "# データおよび中間層の識別補助線を表示\n",
        "x1 = np.linspace(-3, 3)\n",
        "x2 = -1.0 * (b_mid_1 + w_mid_1[0]*x1) / w_mid_1[1]\n",
        "plt.plot(x1, x2, color='k')\n",
        "x2 = -1.0 * (b_mid_2 + w_mid_2[0]*x1) / w_mid_2[1]\n",
        "plt.plot(x1, x2, color='k')\n",
        "for k in range(num_classes):\n",
        "  plt.scatter(X[Y==k,0], X[Y==k,1], color=color_list[k], label='class'+str(k))\n",
        "plt.legend()\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.xlim([-3, 3])\n",
        "plt.ylim([-3, 3])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgNY7XAUtF6Q"
      },
      "source": [
        "中間層の2本の直線によって，非線形な識別境界が作られていることが分かります。  \n",
        "この例では，正の傾きを持つ直線より下で，かつ負の傾きを持つ直線よりも上にいるデータはクラス1（赤），それ以外はクラス0(青)といった感じの識別が行われています。  \n",
        "このような，複数の直線による線形判別を組み合わせた判別は**区分的線形判別**と呼ばれることもあります。  \n",
        "中間層が1層のニューラルネットワークは，近似的に区分的線形判別に等しいことが，この例から分かると思います。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "10_03_neural_network.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}