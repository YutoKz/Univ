{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10_01_softmax_3class.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbqkmaY9RA-z"
      },
      "source": [
        "# 第10回 その1: ソフトマックス関数を用いたロジスティック回帰の多クラス識別への拡張\n",
        "ロジスティック回帰のシグモイド関数をソフトマックス関数に変えることで，3クラス以上の識別を行えるように拡張しましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1DmeW6oRs8r"
      },
      "source": [
        "## ステップ0: Google Driveのマウントと作業フォルダへの移動  \n",
        "Google Drive に配置したデータを読み込むための準備です。  \n",
        "詳細については第二回の 02_01_graph.ipynb を参照してください。  \n",
        "\n",
        "ここでは\"マイドライブ/情報管理/10\"を作業フォルダとします。 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4b4cfHPRXML"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# フォルダの移動には\"%cd\"を使用します。\n",
        "# 作業フォルダへ移動\n",
        "%cd /content/drive/'My Drive'/情報管理/10/\n",
        "# 現在のフォルダの中身を表示\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF6-hyJ2Sdau"
      },
      "source": [
        "`3class_data.csv`というデータが表示されていることを確認してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ステップ1: データの読み込みと標準化\n",
        "まずは必要ライブラリをインポートします。"
      ],
      "metadata": {
        "id": "_zJI0HyoWGGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "SYfdOuD_WNd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`3class_data.csv` を読み込みます。  \n",
        "このデータはx1とx2の二次元データで，クラス0，クラス1，そしてクラス2の3種類のクラスのいずれかに属しています。"
      ],
      "metadata": {
        "id": "SXb_O89VWQQY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkibCXvsRv-8"
      },
      "source": [
        "# pandas の関数 read_csv を用いた csvファイル読み込み\n",
        "csv_data = pd.read_csv('3class_data.csv', encoding='SHIFT-JIS')\n",
        "\n",
        "# データの前半部(.headで取得できる)のみ表示\n",
        "display(csv_data.head())\n",
        "\n",
        "# x1とx2のデータを抽出し，numpy用データ(ndarray型) に変換する。\n",
        "X = csv_data.loc[:, ['x1','x2']].to_numpy()\n",
        "\n",
        "# クラス番号を抽出し，numpy用データ(ndarray型) に変換する。\n",
        "Y = csv_data.loc[:,'class'].to_numpy()\n",
        "\n",
        "# データのサンプル数と次元数を得る。\n",
        "(num_samples, num_dimensions) = np.shape(X)\n",
        "print('Nunber of samples: ' + str(num_samples))\n",
        "print('Number of dimensions: ' + str(num_dimensions))\n",
        "\n",
        "# クラス数を得る。\n",
        "num_classes = int(np.max(Y) + 1)\n",
        "print('Number of classes: ' + str(num_classes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "標準化を行い，データをプロットします。"
      ],
      "metadata": {
        "id": "A2xHCAEBWdLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データを標準化する。\n",
        "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
        "\n",
        "# 以下は color_list = ['b', 'r', 'g'] とほぼ同じ。\n",
        "# 光の三原色(赤,緑,青)の値それぞれを0～1の範囲で定義している。\n",
        "color_list = [(0, 0, 1), (1, 0, 0), (0, 1, 0)]\n",
        "\n",
        "# データをプロット\n",
        "plt.figure(figsize=(7,7))\n",
        "for k in range(num_classes):\n",
        "  # n番目のクラスに属している(Y==c)データをプロット\n",
        "  plt.scatter(X[Y==k,0], X[Y==k,1], color=color_list[k], label='class'+str(k))\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.legend()\n",
        "plt.xlim([-2.5, 2.5])\n",
        "plt.ylim([-2.5, 2.5])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2l_DJKdxWf0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ステップ2: ロジスティック回帰の多クラス識別への拡張\n",
        "二次元のデータを${\\bf x} = [x_1, x_2]$としたとき，ロジスティック回帰は以下の式で定義されていました。  \n",
        "$z = w_1x_1 + w_2x_2 + b = [w_1, w_2]\\left[\\begin{matrix}x_1\\\\x_2 \\end{matrix} \\right] + b = {\\bf wx} + b$    \n",
        "$\\hat{y} = {\\rm sigmoid}(z) = \\frac{1}{1+e^{-z}}$  \n",
        "これに対して，以下の式のようにソフトマックス関数を使って多クラスに拡張します。  \n",
        "（以下は3クラスの場合。添え字は1から始まっていますが，プログラム上は0から始まっている点に注意してください。）  \n",
        "$\\left[\\begin{matrix}z_{1}\\\\z_{2}\\\\z_{3} \\end{matrix} \\right] = \n",
        "\\left[\\begin{matrix} w_{11}x_1 + w_{12}x_2 + b_1 \\\\ w_{21}x_1 + w_{22}x_2 + b_2 \\\\ w_{31}x_1 + w_{32}x_2 + b_3 \\end{matrix} \\right] = \n",
        "\\left[\\begin{matrix} w_{11} & w_{12} \\\\ w_{21} & w_{22} \\\\ w_{31} & w_{32} \\end{matrix} \\right]\\left[\\begin{matrix}x_1\\\\x_2 \\end{matrix} \\right] + \\left[\\begin{matrix}b_{1}\\\\b_{2}\\\\b_{3} \\end{matrix} \\right] = {\\bf W}{\\bf x}+{\\bf b}$  \n",
        "$\\left[\\begin{matrix}\\hat{y}_{1}\\\\\\hat{y}_{2}\\\\\\hat{y}_{3} \\end{matrix} \\right] = \n",
        "\\left[\\begin{matrix}{\\rm softmax}(z_{1})\\\\{\\rm softmax}(z_{2})\\\\{\\rm softmax}(z_{3}) \\end{matrix} \\right] =\n",
        "\\left[\\begin{matrix}\\frac{e^{z_{1}}}{\\sum_{k=1,2,3}e^{z_{k}}} \\\\ \\frac{e^{z_{2}}}{\\sum_{k=1,2,3}e^{z_{k}}} \\\\ \\frac{e^{z_{3}}}{\\sum_{k=1,2,3}e^{z_{k}}}\\end{matrix} \\right]$\n",
        "\n",
        "ロジスティック回帰では$\\bf w$がサイズ=次元数のベクトル，$z$と$\\hat{y}$がスカラーであったのに対して，  \n",
        "今回は$\\bf W$がサイズ=(クラスの数$\\times$次元数)の行列になっており，  \n",
        "従って$\\bf z$と$\\bf \\hat{y}$もサイズ数=クラスの数のベクトルになっています。  \n",
        "また$\\bf \\hat{y}$を求める式はシグモイド関数からソフトマックス関数に変更されています。"
      ],
      "metadata": {
        "id": "kxl2IKgJWiM-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RmuM3aYnanP"
      },
      "source": [
        "1サンプルデータに対して$\\bf \\hat{y}$ を計算する関数を以下に定義します。  \n",
        "08_03_logistic_regression.ipynbのlogistic_regression関数と見比べてみてください。  \n",
        "ロジスティック回帰と今回のマルチクラス回帰ではベクトル/行列，スカラー/ベクトルとパラメータの形式が大きく異なりますが，pythonのnumpyライブラリでは変数がスカラー/ベクトル/行列がどれであっても自動的に処理するため，実装上はsigmoidがsoftmaxに代わっているくらいの差しかありません。  \n",
        "（この点は便利な反面，バグを見落とすリスクもありますが）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkdO-S8SuDPX"
      },
      "source": [
        "def softmax(z):\n",
        "  '''\n",
        "      ソフトマックス関数\n",
        "      z: 要素数=クラス数のベクトル\n",
        "      y_hat: クラス毎の確率。要素数=クラス数のベクトル\n",
        "  '''\n",
        "  return (np.exp(z) / np.sum(np.exp(z)))\n",
        "\n",
        "def multiclass_regression(x, W, b):\n",
        "  '''\n",
        "      1サンプルデータに対してクラス分類を行う\n",
        "      x: 1サンプルデータ。要素数=次元数のベクトル\n",
        "      W: クラス毎に定義されている，各次元に対する重み。サイズ=[クラス数 x 次元数]の行列。\n",
        "         行列Wのk行目のベクトルwkにはk番目のクラスの確率を計算するための重みが格納される。\n",
        "      b: クラス毎に定義されている，切片（バイアス）成分。サイズ=クラス数のベクトル。\n",
        "      y_hat: ソフトマックス回帰の出力。要素数=クラス数のベクトル\n",
        "  '''\n",
        "\n",
        "  # 行列Wとベクトルxの内積はベクトル\n",
        "  # z1 = w11*x1 + w12*x2 + ... + b1\n",
        "  # z2 = w21*x1 + w22*x2 + ... + b2\n",
        "  # ...\n",
        "  z = np.dot(W, x) + b\n",
        "\n",
        "  # ソフトマックス関数に通す\n",
        "  # zおよびy_hatはベクトル\n",
        "  y_hat = softmax(z)\n",
        "\n",
        "  return y_hat\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "重み行列 $\\bf W$ と切片（バイアス）成分 $\\bf b$ の初期値を決めます。  \n",
        "ロジスティック回帰のときと同様，各線形式の切片成分 $\\bf b$ の初期値はゼロとし，重み行列 $\\bf W$ の初期値は正規分布に従う乱数で決めます。"
      ],
      "metadata": {
        "id": "0idvxBemqOeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "# 正規分布に従う乱数行列\n",
        "initial_W = np.random.randn(num_classes, num_dimensions)\n",
        "# ゼロベクトル\n",
        "initial_b = np.zeros(num_classes)\n",
        "\n",
        "print('initial_W = ')\n",
        "print(initial_W)\n",
        "print('initial_b = ')\n",
        "print(initial_b)"
      ],
      "metadata": {
        "id": "iON3maL-o9AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "さて，この状態で一度クラス分類を実行してみましょう。  \n",
        "例として5番目のサンプルデータに対してクラス識別を行います。  \n",
        "$\\bf \\hat{y}$は要素数=クラス数のベクトルで，それぞれそのクラスの確率値が入っています。  \n",
        "そこで，確率値が最大のクラスを識別結果として出力します。  \n",
        "\n",
        "また，各クラスの線 $z_{k} = w_{k1}x_1 + w_{k2}x_2 + b_{k} = 0$ をプロットしてみます。  \n",
        "ただし $x_2 = -(b_{k} + w_{k1}x_1) / w_{k2}$  \n",
        "と変形したものをプロットします。  "
      ],
      "metadata": {
        "id": "OViMDQCAZcUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5番目のデータで試す\n",
        "sample_id = 5\n",
        "x = X[sample_id,:]\n",
        "y = Y[sample_id]\n",
        "\n",
        "# 初期値をコピー\n",
        "W = initial_W.copy()\n",
        "b = initial_b.copy()\n",
        "\n",
        "# データとwの線をプロットしてみる。\n",
        "color_list = [(0, 0, 1), (1, 0, 0), (0, 1, 0)]\n",
        "# データをプロット\n",
        "plt.figure(figsize=(7,7))\n",
        "\n",
        "x1 = np.linspace(-3, 3)\n",
        "for k in range(num_classes):\n",
        "  x2 = -1.0 * (initial_b[k] + initial_W[k, 0]*x1) / initial_W[k, 1]\n",
        "  plt.plot(x1, x2, c=color_list[k])\n",
        "  plt.scatter(X[Y==k,0], X[Y==k,1], color=color_list[k], label='class'+str(k))\n",
        "plt.scatter(x[0], x[1], c='m', marker='x', s=100, label='tested sample')\n",
        "plt.legend()\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.xlim([-2.5, 2.5])\n",
        "plt.ylim([-2.5, 2.5])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# マルチクラス回帰関数に与える\n",
        "y_hat = multiclass_regression(x, W, b)\n",
        "\n",
        "# 確率値が最大となるクラス番号を出力する\n",
        "pred = np.argmax(y_hat)\n",
        "\n",
        "# 結果を出力する\n",
        "print('y_hat = ' + str(y_hat))\n",
        "print('estimated label = ' + str(pred))\n",
        "print('true label = ' + str(y))"
      ],
      "metadata": {
        "id": "TB_Mx3knvEd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "y_hatは1番目の確率が最も高かったため，estimated label（識別結果）は1となりました。  \n",
        "一方true label(正解ラベル)は0なので，この識別結果は誤りです。  "
      ],
      "metadata": {
        "id": "Q7x1npYYa_Hk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "次に，このときの勾配を計算してみます。"
      ],
      "metadata": {
        "id": "MNuvAlY5bSBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "まず，勾配を計算する関数を以下に定義します。   \n",
        "ここではクロスエントロピーを損失関数として用います。  \n",
        "クロスエントロピーとソフトマックス関数を用いた場合の勾配は以下の式で定義されます。   \n",
        "$\\frac{\\partial L_{ce}}{\\partial \\bf w_k} = (\\hat{y}_k - y_k){\\bf x}$  \n",
        "$\\frac{\\partial L_{ce}}{\\partial \\bf b_k} = \\hat{y}_k - y_k$  \n",
        "$k$はクラス番号です。  \n",
        "$\\hat{y}_k$はソフトマックス関数が出力した，クラス$k$の確率です。  \n",
        "$y_k$は正解ラベルで，$k$が正解クラスの場合は確率1，不正解クラスの場合は0です。  "
      ],
      "metadata": {
        "id": "gu0ihSZ-tPcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(y_hat, y):\n",
        "  '''\n",
        "      クロスエントロピー損失を計算する。\n",
        "      y_hat: マルチクラス回帰によって推定された y\n",
        "      y: 正解のクラス\n",
        "  '''\n",
        "  # 正解クラスの確率を1，それ以外のクラスの確率を0とするベクトル（one-hotベクトル）を作成する。\n",
        "  y_onehot = np.zeros(num_classes)\n",
        "  y_onehot[y] = 1.0\n",
        "\n",
        "  ce = -1.0 * np.sum(y_onehot*np.log(y_hat))\n",
        "\n",
        "  return ce\n",
        "\n",
        "def calc_gradient(y_hat, y, x):\n",
        "  '''\n",
        "      勾配を計算する\n",
        "      y_hat: マルチクラス回帰によって推定された y。サイズ=Kのベクトル\n",
        "      y: 正解のクラス。スカラ\n",
        "      x: データ。サイズ=次元数のベクトル\n",
        "      grad_W: Wの勾配：wの数だけ存在。つまりサイズ=(KxD)の行列\n",
        "      grad_b: bの勾配：bの数だけ存在。つまりサイズ=Kのベクトル\n",
        "      ただし，D:次元数，K:クラス数である。\n",
        "  '''\n",
        "  # 次元数D, クラス数Kを得る。\n",
        "  D = np.size(x)\n",
        "  K = np.size(y_hat)\n",
        "\n",
        "  # 正解クラスの確率を1，それ以外のクラスの確率を0とするベクトル（one-hotベクトル）を作成する。\n",
        "  y_onehot = np.zeros(K)\n",
        "  y_onehot[y] = 1.0\n",
        "\n",
        "  # 勾配を計算\n",
        "  grad_W = np.zeros([K, D])\n",
        "  grad_b = np.zeros(K)\n",
        "  for k in range(K):\n",
        "    grad_W[k,:] = (y_hat[k] - y_onehot[k]) * x\n",
        "    grad_b[k] = y_hat[k] - y_onehot[k]\n",
        "\n",
        "  return grad_W, grad_b"
      ],
      "metadata": {
        "id": "1G33yRYprRsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "定義した関数を使って，勾配を計算します。"
      ],
      "metadata": {
        "id": "hoH7Ox-M0aYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grad_W, grad_b = calc_gradient(y_hat, y, x)\n",
        "print('gradient of L for W = ')\n",
        "print(grad_W)\n",
        "print('gradient of L for b = ')\n",
        "print(grad_b)"
      ],
      "metadata": {
        "id": "yTZrTs3KvlYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "計算された勾配を使って，  \n",
        "${\\bf W}_{\\rm next} = {\\bf W} - \\mu \\frac{\\partial L_{ce}}{\\partial \\bf W}$  \n",
        "${\\bf b}_{\\rm next} = {\\bf b} - \\mu \\frac{\\partial L_{ce}}{\\partial \\bf b}$  \n",
        "により，${\\bf W}$ および $\\bf b$ を更新してみます。  \n",
        "ここでは，$\\mu = 0.2$とします。"
      ],
      "metadata": {
        "id": "Hjt2siNKcJhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習率(lr = learning rate)\n",
        "lr = 0.2\n",
        "\n",
        "# 勾配を使って Wおよびb を更新\n",
        "W_next = W - lr * grad_W\n",
        "b_next = b - lr * grad_b\n",
        "\n",
        "\n",
        "color_list = [(0, 0, 1), (1, 0, 0), (0, 1, 0)]\n",
        "# データをプロット\n",
        "plt.figure(figsize=(7,7))\n",
        "\n",
        "x1 = np.linspace(-3, 3)\n",
        "for k in range(num_classes):\n",
        "  x2 = -1.0 * (initial_b[k] + initial_W[k, 0]*x1) / initial_W[k, 1]\n",
        "  x2next = -1.0 * (b_next[k] + W_next[k, 0]*x1) / W_next[k, 1]\n",
        "  plt.plot(x1, x2, c=color_list[k])\n",
        "  plt.plot(x1, x2next, c=color_list[k], linestyle = \":\")\n",
        "  plt.scatter(X[Y==k,0], X[Y==k,1], color=color_list[k], label='class'+str(k))\n",
        "plt.scatter(x[0], x[1], c='m', marker='x', s=100, label='tested sample')\n",
        "plt.legend()\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.xlim([-2.5, 2.5])\n",
        "plt.ylim([-2.5, 2.5])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IVSfCsDp0hlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ロジスティック回帰のときと違い，3本の線が同時に更新されました。  \n",
        "実線が更新前，点線が更新後です。  \n",
        "見ると，赤線と緑線（不正解クラス）の線はテストデータ(x印)から離れ，青線（正解クラス）の線はテストデータに近づいていることが分かります。  \n",
        "\n",
        "更新前と更新後のマルチクラス回帰の出力$\\bf \\hat{y}$を見比べてみましょう。  \n"
      ],
      "metadata": {
        "id": "o_hJk_731p2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat_old = multiclass_regression(x, W, b)\n",
        "y_hat_next = multiclass_regression(x, W_next, b_next)\n",
        "print('y_hat_old = ' + str(y_hat_old))\n",
        "print('y_hat_next = ' + str(y_hat_next))"
      ],
      "metadata": {
        "id": "2vQXg694dVfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "更新により，$\\bf \\hat{y}$は0番目のクラス，つまり正解クラスの確率が高くなり，1番目と2番目，つまり不正解クラスの確率は低くなりました。  \n",
        "結果として今回の更新により，このデータに関しては正しく識別できるようになったことになります。  "
      ],
      "metadata": {
        "id": "VVv8mWCFdtPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ではロジスティック回帰のときと同様に，全てのデータを用いて繰り返し更新しましょう。   \n",
        "ここでは全データサンプルに対して更新を行うひとまとまりの処理を10周（エポック）行っています。"
      ],
      "metadata": {
        "id": "TKI-bcdJXlhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import animation, rc\n",
        "from IPython.display import HTML\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "# 学習率\n",
        "lr = 0.2\n",
        "\n",
        "# 重みおよびバイアスの初期化\n",
        "W = initial_W.copy()\n",
        "b = initial_b.copy()\n",
        "\n",
        "# 描画\n",
        "color_list = [(0, 0, 1), (1, 0, 0), (0, 1, 0)]\n",
        "fig = plt.figure(figsize=(7,7))\n",
        "# データの描画\n",
        "for k in range(num_classes):\n",
        "  plt.scatter(X[Y==k,0], X[Y==k,1], color=color_list[k], label='class'+str(k))\n",
        "images = []\n",
        "\n",
        "# 損失関数の履歴\n",
        "loss_history = np.array([])\n",
        "# 正解率の履歴\n",
        "acc_history = np.array([])\n",
        "\n",
        "for epoch in range(10):\n",
        "  \n",
        "  # データをシャッフルしなおす。\n",
        "  # (局所解に陥りにくくなるため)\n",
        "  shuffle_index = np.random.permutation(np.arange(num_samples))\n",
        "  X_tmp = X[(shuffle_index)]\n",
        "  Y_tmp = Y[(shuffle_index)]\n",
        "  \n",
        "  loss = 0\n",
        "  acc = 0\n",
        "  for n in range(num_samples):\n",
        "    x = X_tmp[n,:]\n",
        "    y = Y_tmp[n]\n",
        "  \n",
        "    # 回帰関数に与える\n",
        "    y_hat = multiclass_regression(x, W, b)\n",
        "  \n",
        "    # 勾配の計算\n",
        "    grad_W, grad_b = calc_gradient(y_hat, y, x)\n",
        "\n",
        "    # 更新スピードの速い1エポックでは50サンプルごと，それ以外は200サンプル\n",
        "    # ごとにアニメーション描画する。\n",
        "    if (epoch < 2 and n % 50 == 0) or (epoch >=2 and n % 200 == 0):\n",
        "      img = []\n",
        "      for k in range(num_classes):\n",
        "        x2 = -1.0 * (b[k] + W[k, 0]*x1) / W[k, 1]\n",
        "        img += plt.plot(x1, x2, c=color_list[k])\n",
        "      img.append(plt.text(-2.3, 2.3, 'epoch: '+str(epoch), size='x-large'))\n",
        "      images.append(img)\n",
        "\n",
        "    # 更新\n",
        "    W -= lr * grad_W\n",
        "    b -= lr * grad_b\n",
        "\n",
        "    # 損失関数の蓄積\n",
        "    loss += cross_entropy(y_hat, y)\n",
        "    # 正解率の蓄積\n",
        "    if np.argmax(y_hat) == y:\n",
        "      acc += 1\n",
        "  \n",
        "  loss /= num_samples\n",
        "  acc /= num_samples\n",
        "  loss_history = np.append(loss_history, loss)\n",
        "  acc_history = np.append(acc_history, acc*100)\n",
        "  print('%d-th epoch: cross entropy = %.3f, accuracy = %.3f%%' % (epoch+1, loss, acc*100))\n",
        "\n",
        "plt.legend()\n",
        "plt.xlim([-2.5, 2.5])\n",
        "plt.ylim([-2.5, 2.5])\n",
        "\n",
        "# アニメーション作成\n",
        "anim = animation.ArtistAnimation(fig, images, interval=200)\n",
        "# Google Colaboratoryの場合必要\n",
        "rc('animation', html='jshtml')\n",
        "plt.close()\n",
        "display(anim)"
      ],
      "metadata": {
        "id": "6YmZ596h0-lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "エポック毎の損失関数（クロスエントロピー）および分類正解率をプロットします。  "
      ],
      "metadata": {
        "id": "je4-8YEccEJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss (cross entropy)')\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(acc_history)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy [%]')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nIjzgawo2NbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "（エポック毎に計算していますが）学習完了後の分類正解率を計算します。  "
      ],
      "metadata": {
        "id": "5OL88EjpdRZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 正解率\n",
        "accuracy = 0\n",
        "\n",
        "for n in range(num_samples):\n",
        "  x = X[n,:]\n",
        "  y = Y[n]\n",
        "  # マルチクラス回帰関数に与える\n",
        "  y_hat = multiclass_regression(x, W, b)\n",
        "\n",
        "  if np.argmax(y_hat) == y:\n",
        "    accuracy += 1\n",
        "\n",
        "accuracy = 100.0 * accuracy / num_samples\n",
        "\n",
        "print('Accuracy = %.3f' % (accuracy))"
      ],
      "metadata": {
        "id": "8KgbvmdvcwPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "100%の正解率が得られました。"
      ],
      "metadata": {
        "id": "RztiWfSZeRPZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "最後に，クラス毎の識別境界を可視化します。  \n",
        "上記で描画した3本の線は識別境界ではなく，あくまで識別境界を求めるための補助線です。  \n",
        "ここでは，まず，x1，x2の値を(-2.6～2.6)までの範囲で200点に区切ることで格子点(grid)を作成します。  \n",
        "そして格子点ごとクラス識別を行うことで，(-2.6～2.6)の範囲内に色塗りを行います。  "
      ],
      "metadata": {
        "id": "uvpCbnN4nzwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x1，x2ごとの格子点の数\n",
        "grid_size = 200\n",
        "\n",
        "# 格子点：-2.6から2.6まで200個分，等間隔に区切った数列\n",
        "grid = np.linspace(-2.6, 2.6, grid_size)\n",
        "\n",
        "# クラス識別結果を色塗りする領域：200x200の図\n",
        "# 最後の3は，色情報(RGB=赤・緑・青の3次元ごとの色の強さ)\n",
        "image = np.zeros([grid_size, grid_size, 3])\n",
        "\n",
        "color_list = [(0, 0, 1), (1, 0, 0), (0, 1, 0)]\n",
        "\n",
        "for i in range(grid_size):\n",
        "  for j in range(grid_size):\n",
        "    # 格子点のデータ\n",
        "    x = np.array([grid[j], grid[i]])\n",
        "    # 格子点データを識別する\n",
        "    y_hat = multiclass_regression(x, W, b)\n",
        "    label = np.argmax(y_hat)\n",
        "    # 識別結果のクラスの色で，色を塗る\n",
        "    # (グラフは原点が左下だが，画像は左上が原点なので，上下反転させるために-iとしている)\n",
        "    image[-i, j, :] = color_list[label]\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "# 色塗り結果を表示\n",
        "plt.imshow(image, alpha=0.4, extent=(-2.6, 2.6, -2.6, 2.6))\n",
        "# データおよび3本線を表示\n",
        "for k in range(num_classes):\n",
        "  x2 = -1.0 * (b[k] + W[k, 0]*x1) / W[k, 1]\n",
        "  plt.plot(x1, x2, c=color_list[k])\n",
        "  plt.scatter(X[Y==k,0], X[Y==k,1], color=color_list[k], label='class'+str(k))\n",
        "plt.legend()\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.xlim([-2.5, 2.5])\n",
        "plt.ylim([-2.5, 2.5])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1gjbinbByfQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "各クラスの識別領域が色付けされ，それにより識別境界が可視化されました。  \n",
        "また，例えば緑の補助線と青の補助線の間に緑クラスと青クラスの境界線ができているというように，3本の補助線の間を取るような形で識別境界が作られていることが分かります。  "
      ],
      "metadata": {
        "id": "wgNY7XAUtF6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_f2tKsC8ae22"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}